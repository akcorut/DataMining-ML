{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tensorflow.keras import backend\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import collections\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from pathlib import Path\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from six import StringIO\n",
    "from copy import copy\n",
    "from scipy import stats\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def encodeWithBLOSUM62(amino_acids):\n",
    "    ## a function that returns the blosum62 vector given a certain aa \n",
    "    return list(BLOSUM62_MATRIX[amino_acids].values)\n",
    "\n",
    "def blosumEncoding(data):\n",
    "    ## a function that creates amino acid sequence that encode by blosum62 matrix \n",
    "    total_data_row =[]\n",
    "    for each in data:\n",
    "    \n",
    "        eachrow =[]\n",
    "        for aa in each:\n",
    "            eachrow = eachrow+ encodeWithBLOSUM62(aa)\n",
    "    \n",
    "        total_data_row.append(eachrow)\n",
    "    \n",
    "    return pd.DataFrame(total_data_row)\n",
    "\n",
    "def split(word): \n",
    "    return [char for char in word] \n",
    "\n",
    "COMMON_AMINO_ACIDS = collections.OrderedDict(sorted({\n",
    "   \"A\": \"Alanine\",\n",
    "   \"R\": \"Arginine\",\n",
    "   \"N\": \"Asparagine\",\n",
    "   \"D\": \"Aspartic Acid\",\n",
    "   \"C\": \"Cysteine\",\n",
    "   \"E\": \"Glutamic Acid\",\n",
    "   \"Q\": \"Glutamine\",\n",
    "   \"G\": \"Glycine\",\n",
    "   \"H\": \"Histidine\",\n",
    "   \"I\": \"Isoleucine\",\n",
    "   \"L\": \"Leucine\",\n",
    "   \"K\": \"Lysine\",\n",
    "   \"M\": \"Methionine\",\n",
    "   \"F\": \"Phenylalanine\",\n",
    "   \"P\": \"Proline\",\n",
    "   \"S\": \"Serine\",\n",
    "   \"T\": \"Threonine\",\n",
    "   \"W\": \"Tryptophan\",\n",
    "   \"Y\": \"Tyrosine\",\n",
    "   \"V\": \"Valine\",\n",
    "   }.items()))\n",
    "COMMON_AMINO_ACIDS_WITH_UNKNOWN = copy(COMMON_AMINO_ACIDS)\n",
    "COMMON_AMINO_ACIDS_WITH_UNKNOWN[\"X\"] = \"Unknown\"\n",
    "   \n",
    "AMINO_ACID_INDEX = dict((letter, i) for (i, letter) in enumerate(COMMON_AMINO_ACIDS_WITH_UNKNOWN))\n",
    "\n",
    "AMINO_ACIDS = list(COMMON_AMINO_ACIDS_WITH_UNKNOWN.keys())\n",
    "   \n",
    "BLOSUM62_MATRIX = pd.read_csv(StringIO(\"\"\"\n",
    "  A  R  N  D  C  Q  E  G  H  I  L  K  M  F  P  S  T  W  Y  V  X\n",
    "   A  4 -1 -2 -2  0 -1 -1  0 -2 -1 -1 -1 -1 -2 -1  1  0 -3 -2  0  0\n",
    "   R -1  5  0 -2 -3  1  0 -2  0 -3 -2  2 -1 -3 -2 -1 -1 -3 -2 -3  0\n",
    "   N -2  0  6  1 -3  0  0  0  1 -3 -3  0 -2 -3 -2  1  0 -4 -2 -3  0\n",
    "   D -2 -2  1  6 -3  0  2 -1 -1 -3 -4 -1 -3 -3 -1  0 -1 -4 -3 -3  0\n",
    "   C  0 -3 -3 -3  9 -3 -4 -3 -3 -1 -1 -3 -1 -2 -3 -1 -1 -2 -2 -1  0\n",
    "   Q -1  1  0  0 -3  5  2 -2  0 -3 -2  1  0 -3 -1  0 -1 -2 -1 -2  0\n",
    "   E -1  0  0  2 -4  2  5 -2  0 -3 -3  1 -2 -3 -1  0 -1 -3 -2 -2  0\n",
    "   G  0 -2  0 -1 -3 -2 -2  6 -2 -4 -4 -2 -3 -3 -2  0 -2 -2 -3 -3  0\n",
    "   H -2  0  1 -1 -3  0  0 -2  8 -3 -3 -1 -2 -1 -2 -1 -2 -2  2 -3  0\n",
    "   I -1 -3 -3 -3 -1 -3 -3 -4 -3  4  2 -3  1  0 -3 -2 -1 -3 -1  3  0\n",
    "   L -1 -2 -3 -4 -1 -2 -3 -4 -3  2  4 -2  2  0 -3 -2 -1 -2 -1  1  0\n",
    "   K -1  2  0 -1 -3  1  1 -2 -1 -3 -2  5 -1 -3 -1  0 -1 -3 -2 -2  0\n",
    "   M -1 -1 -2 -3 -1  0 -2 -3 -2  1  2 -1  5  0 -2 -1 -1 -1 -1  1  0\n",
    "   F -2 -3 -3 -3 -2 -3 -3 -3 -1  0  0 -3  0  6 -4 -2 -2  1  3 -1  0\n",
    "   P -1 -2 -2 -1 -3 -1 -1 -2 -2 -3 -3 -1 -2 -4  7 -1 -1 -4 -3 -2  0\n",
    "   S  1 -1  1  0 -1  0  0  0 -1 -2 -2  0 -1 -2 -1  4  1 -3 -2 -2  0\n",
    "   T  0 -1  0 -1 -1 -1 -1 -2 -2 -1 -1 -1 -1 -2 -1  1  5 -2 -2  0  0 \n",
    "   W -3 -3 -4 -4 -2 -2 -3 -2 -2 -3 -2 -3 -1  1 -4 -3 -2 11  2 -3  0\n",
    "   Y -2 -2 -2 -3 -2 -1 -2 -3  2 -1 -1 -2 -1  3 -3 -2 -2  2  7 -1  0\n",
    "   V  0 -3 -3 -3 -1 -2 -2 -3 -3  3  1 -2  1 -1 -2 -2  0 -3 -1  4  0\n",
    "   X  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
    "   \"\"\"), sep='\\s+').loc[AMINO_ACIDS, AMINO_ACIDS].astype(\"int8\")\n",
    "assert (BLOSUM62_MATRIX == BLOSUM62_MATRIX.T).all().all() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSameLength(each_peptide):\n",
    "    first = each_peptide\n",
    "    middle = each_peptide.center(15,\"X\")\n",
    "    end = ''\n",
    "    \n",
    "    while len(first) < 15:\n",
    "        first +='X'\n",
    "        end+='X' \n",
    "    end = end + each_peptide[-len(each_peptide):]\n",
    "    return first+middle+end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data preprocessing, importing the dataset\n",
    "## need to change the file location\n",
    "workdir = Path('/Users/kivanc/DataMining-ML/Projects/FinalProject')\n",
    "               #'/Volumes/Research/MAC_Research_Data/UGA/3Fall/DataMining6380/Mini_projecct_2/')\n",
    "dataset = pd.read_csv(workdir/\"HLA-A-01.txt\",sep='\\t')\n",
    "\n",
    "peptides = dataset.peptide.values\n",
    "final_peptide = [split(makeSameLength(each_peptide)) for each_peptide in peptides]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode y label\n",
    "from sklearn import preprocessing\n",
    "y = dataset.Binder.values\n",
    "le = preprocessing.LabelEncoder()\n",
    "a = le.fit(y)\n",
    "y= le.transform(y)\n",
    "\n",
    "## encode x label using bl62 matrix\n",
    "X = blosumEncoding(final_peptide).values\n",
    "## to see the dataset distribution \n",
    "dataset.groupby('Binder').size()\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split \n",
    "val_size = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, shuffle= True,stratify = y)\n",
    "X_train, X_val, y_train, y_val =  train_test_split(X_train, y_train, test_size=val_size, random_state = 0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## balance the class and create a new dataframe\n",
    "from sklearn.utils import resample\n",
    "Binder_max_count = dataset.groupby(['Binder']).count().max()[0]\n",
    "Binders = dataset.Binder.unique()\n",
    "Binders = np.delete(Binders, np.where(Binders == False), axis =0 )\n",
    "df_majority = dataset[dataset.Binder==False]\n",
    "for binder in Binders: \n",
    "    df_minority_upsampled = resample(dataset[dataset['Binder']==binder], \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=Binder_max_count,    # to match majority class\n",
    "                                 random_state=42) # reproducible results\n",
    "    \n",
    "    df_majority = pd.concat([df_majority, df_minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Balance the classes\n",
    "peptides = df_majority.peptide.values\n",
    "final_peptide = [split(makeSameLength(each_peptide)) for each_peptide in peptides]\n",
    "\n",
    "y = df_majority.Binder.values\n",
    "le = preprocessing.LabelEncoder()\n",
    "a = le.fit(y)\n",
    "y= le.transform(y)\n",
    "\n",
    "## encode x label using bl62 matrix\n",
    "X = blosumEncoding(final_peptide).values\n",
    "## to see the df_majority distribution \n",
    "df_majority.groupby('Binder').size()\n",
    "# Splitting the df_majority into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split \n",
    "val_size = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, shuffle= True,stratify = y)\n",
    "X_train, X_val, y_train, y_val =  train_test_split(X_train, y_train, test_size=val_size, random_state = 0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_83 (Dense)             (None, 80)                75680     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 2)                 162       \n",
      "=================================================================\n",
      "Total params: 88,802\n",
      "Trainable params: 88,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an ensemble Neural Network in Keras\n",
    "\n",
    "## Create an ANN model\n",
    "# Training the model\n",
    "# Dense object will take care to initialize the random number close to 0 ( first ANN step)\n",
    "classifier = Sequential() # use the sequential layer\n",
    "classifier.add(Dense(units = 80, kernel_initializer = 'uniform', activation = 'tanh', input_dim = 945))\n",
    "classifier.add(Dropout(rate = 0.5))\n",
    "classifier.add(Dense(units = 80,  kernel_initializer = 'uniform', activation = 'tanh', input_dim = 945))\n",
    "classifier.add(Dropout(rate = 0.5))\n",
    "classifier.add(Dense(units = 80,   kernel_initializer = 'uniform', activation = 'tanh', input_dim = 945))\n",
    "## here is the output layer\n",
    "## if we deal with more than 2 categories, the activation function needs to use softmax\n",
    "classifier.add(Dense(units = 2, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "opt = keras.optimizers.rmsprop(learning_rate= 0.001)\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting the ANN to the Training set\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience= 50)\n",
    "# check the model performance and save the best model\n",
    "mc = ModelCheckpoint(\n",
    "    'D:/GradSchool/Classes/DataMining/FinalProject_NN/Final_best_50_batch_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "    #'/Volumes/Research/MAC_Research_Data/UGA/3Fall/DataMining6380/Mini_projecct_2/Minproject2best_50batch_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train, y_train):\n",
    "    #define model\n",
    "    classifier = Sequential() # use the sequential layer\n",
    "    classifier.add(Dense(units = 80, kernel_initializer = 'uniform', activation = 'tanh', input_dim = 945))\n",
    "    classifier.add(Dropout(rate = 0.5))\n",
    "    classifier.add(Dense(units = 80,  kernel_initializer = 'uniform', activation = 'tanh', input_dim = 945))\n",
    "    classifier.add(Dropout(rate = 0.5))\n",
    "    classifier.add(Dense(units = 80,   kernel_initializer = 'uniform', activation = 'tanh', input_dim = 945))\n",
    "    ## here is the output layer\n",
    "    ## if we deal with more than 2 categories, the activation function needs to use softmax\n",
    "    \n",
    "    #I changed the output layer to have 2 units\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    opt = keras.optimizers.rmsprop(learning_rate= 0.001)\n",
    "    # Compiling the ANN\n",
    "    classifier.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    # Fitting the ANN to the Training set\n",
    "    # simple early stopping\n",
    "    classifier.fit(X_train, y_train, epochs=500, verbose=0)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "\n",
    "# Delete the folder and start here if I want to do a clean run\n",
    "\n",
    "# create directory for models\n",
    "makedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Saved models/model_1.h5\n",
      ">Saved models/model_2.h5\n",
      ">Saved models/model_3.h5\n",
      ">Saved models/model_4.h5\n",
      ">Saved models/model_5.h5\n"
     ]
    }
   ],
   "source": [
    "# fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "    # fit model\n",
    "    model = fit_model(X_train, y_train)\n",
    "    # save model\n",
    "    filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "    model.save(filename)\n",
    "    print('>Saved %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the separate stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "from keras.models import load_model\n",
    "\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded models/model_1.h5\n",
      ">loaded models/model_2.h5\n",
      ">loaded models/model_3.h5\n",
      ">loaded models/model_4.h5\n",
      ">loaded models/model_5.h5\n",
      "Loaded 5 models\n"
     ]
    }
   ],
   "source": [
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.922\n",
      "Model Accuracy: 0.922\n",
      "Model Accuracy: 0.919\n",
      "Model Accuracy: 0.919\n",
      "Model Accuracy: 0.922\n"
     ]
    }
   ],
   "source": [
    "# evaluate standalone models on test dataset\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "for model in members:\n",
    "    #testy_enc = to_categorical(y_test)\n",
    "    _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stacked model input dataset as outputs from the ensemble\n",
    "import numpy as np\n",
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    for model in members:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model based on the outputs from the ensemble members\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit standalone model\n",
    "    #model = LogisticRegression()\n",
    "    #model = DecisionTreeClassifier()\n",
    "    #model = GaussianNB()\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=18, max_features = 'sqrt',n_jobs=-1, verbose = 0)\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, X_test)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two other classifiers with kfold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each fold accuracy: 0.4794841735052755\n",
      "each fold accuracy: 0.4912075029308324\n",
      "each fold accuracy: 0.738569753810082\n",
      "each fold accuracy: 0.7866354044548651\n",
      "each fold accuracy: 0.8253223915592028\n",
      "each fold accuracy: 0.7702227432590856\n",
      "each fold accuracy: 0.8464243845252052\n",
      "each fold accuracy: 0.8218053927315357\n",
      "each fold accuracy: 0.784037558685446\n",
      "each fold accuracy: 0.5199530516431925\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# xgboost\n",
    "xgbscores = []\n",
    "clf_xgb = XGBClassifier()\n",
    "k_fold = StratifiedKFold(n_splits=10)\n",
    "for train_indices, test_indices in k_fold.split(X,y):\n",
    "    clf_xgb.fit(X[train_indices], y[train_indices]) \n",
    "    predicted_dt = clf_xgb.predict(X[test_indices])\n",
    "    xgbscores.append(accuracy_score(y[test_indices], predicted_dt))\n",
    "    \n",
    "for i in xgbscores:\n",
    "    print(\"each fold accuracy:\", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each fold accuracy: 0.8147713950762017\n",
      "each fold accuracy: 0.7713950762016413\n",
      "each fold accuracy: 0.8053927315357562\n",
      "each fold accuracy: 0.776084407971864\n",
      "each fold accuracy: 0.7784290738569754\n",
      "each fold accuracy: 0.8065650644783119\n",
      "each fold accuracy: 0.8194607268464243\n",
      "each fold accuracy: 0.794841735052755\n",
      "each fold accuracy: 0.7946009389671361\n",
      "each fold accuracy: 0.8204225352112676\n"
     ]
    }
   ],
   "source": [
    "# Ada Boost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "abcscores = []\n",
    "clf_abc = AdaBoostClassifier(n_estimators = 80, learning_rate = .001)\n",
    "k_fold = StratifiedKFold(n_splits=10)\n",
    "for train_indices, test_indices in k_fold.split(X,y):\n",
    "    clf_abc.fit(X[train_indices], y[train_indices]) \n",
    "    predicted_dt = clf_abc.predict(X[test_indices])\n",
    "    abcscores.append(accuracy_score(y[test_indices], predicted_dt))\n",
    "    \n",
    "for i in abcscores:\n",
    "    print(\"each fold accuracy:\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Corey\\Anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_30 to have shape (1,) but got array with shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-a36329918743>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_ix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_ix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'>%.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-a36329918743>\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(trainX, trainy, testX, testy)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# simple early stopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesty_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_30 to have shape (1,) but got array with shape (2,)"
     ]
    }
   ],
   "source": [
    "# kfold cross validation with our NN\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import numpy\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "\n",
    "# evaluate a single mlp model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # encode targets\n",
    "    trainy_enc = to_categorical(trainy)\n",
    "    testy_enc = to_categorical(testy)\n",
    "    # define model\n",
    "    model = Sequential() # use the sequential layer\n",
    "    model.add(Dense(units = 80, kernel_initializer = 'uniform', activation = 'tanh', input_dim = 945))\n",
    "    model.add(Dropout(rate = 0.5))\n",
    "    model.add(Dense(units = 80,  kernel_initializer = 'uniform', activation = 'tanh', input_dim = 945))\n",
    "    model.add(Dropout(rate = 0.5))\n",
    "    model.add(Dense(units = 80,   kernel_initializer = 'uniform', activation = 'tanh', input_dim = 945))\n",
    "    ## here is the output layer\n",
    "    ## if we deal with more than 2 categories, the activation function needs to use softmax\n",
    "    \n",
    "    #I changed the output layer to have 2 units\n",
    "    model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    opt = keras.optimizers.rmsprop(learning_rate= 0.001)\n",
    "    # Compiling the ANN\n",
    "    model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    # Fitting the ANN to the Training set\n",
    "    # simple early stopping\n",
    "    model.fit(X_train, y_train, epochs=500, verbose=0)\n",
    "    _, test_acc = model.evaluate(testX, testy_enc, verbose=0)\n",
    "    return model, test_acc\n",
    " \n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "    # sum across ensemble members\n",
    "    summed = numpy.sum(yhats, axis=0)\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "    return result\n",
    " \n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)\n",
    " \n",
    "\n",
    "# prepare the k-fold cross-validation configuration\n",
    "n_folds = 10\n",
    "kfold = KFold(n_folds, True, 1)\n",
    "# cross validation estimation of performance\n",
    "scores, members = list(), list()\n",
    "for train_ix, test_ix in kfold.split(X):\n",
    "    # select samples\n",
    "    trainX, trainy = X[train_ix], y[train_ix]\n",
    "    testX, testy = X[test_ix], y[test_ix]\n",
    "    # evaluate model\n",
    "    model, test_acc = evaluate_model(trainX, trainy, testX, testy)\n",
    "    print('>%.3f' % test_acc)\n",
    "    scores.append(test_acc)\n",
    "    members.append(model)\n",
    "# summarize expected performance\n",
    "print('Estimated Accuracy %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, n_folds+1):\n",
    "    ensemble_score = evaluate_n_members(members, i, newX, newy)\n",
    "    newy_enc = to_categorical(newy)\n",
    "    _, single_score = members[i-1].evaluate(newX, newy_enc, verbose=0)\n",
    "    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "    ensemble_scores.append(ensemble_score)\n",
    "    single_scores.append(single_score)\n",
    "# plot score vs number of ensemble members\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n",
    "x_axis = [i for i in range(1, n_folds+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# This next step somehow hurts the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start the Integrated Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update all layers in all models to not be trainable\n",
    "for i in range(len(members)):\n",
    "    model = members[i]\n",
    "    for layer in model.layers:\n",
    "        # make not trainable\n",
    "        layer.trainable = False\n",
    "        # rename to avoid 'unique layer name' issue\n",
    "        layer._name = 'ensemble_' + str(i+1) + '_' + layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from matplotlib import pyplot\n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(units = 80, activation = 'tanh')(merge)\n",
    "    output = Dense(units = 1, activation = 'sigmoid')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # plot graph of ensemble\n",
    "    plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "    # compile\n",
    "    model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ensemble model\n",
    "\n",
    "stacked_model = define_stacked_model(members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    # encode output data\n",
    "    #inputy_enc = to_categorical(inputy)\n",
    "    # fit model\n",
    "    model.fit(X, inputy, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    # make prediction\n",
    "    return model.predict(X, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-40cc160cac6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#yhat = argmax(yhat, axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Integrated Stacked Test Accuracy: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 91\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "from numpy import array\n",
    "\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, X_test)\n",
    "#yhat = argmax(yhat, axis=1)\n",
    "yhat = array(yhat)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Integrated Stacked Test Accuracy: %.3f' % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-f0d1bf7b2b36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "yhat = stacked_prediction(members, model, X_test)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}