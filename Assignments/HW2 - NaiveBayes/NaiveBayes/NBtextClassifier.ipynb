{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600436905080",
   "display_name": "Python 3.8.5 64-bit ('data_mining': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CSCI6380 Data Mining: Assignment #2\n",
    "### Adnan Kivanc Corut\n",
    "#### September 24, 2020"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/kivanc/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stops = stopwords.words('english') + list(string.punctuation) + list(string.digits)"
   ]
  },
  {
   "source": [
    "## Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_guthenberg(files):\n",
    "    \"\"\"\n",
    "    This function tries to remove the generic \n",
    "    words at the start of the text files from \n",
    "    the Project Gutenberg ebooks.\n",
    "    \"\"\"    \n",
    "    for f in files:\n",
    "        lines = open(f, 'r').readlines()\n",
    "\n",
    "        strings = (\"***\", \"<<<<<<<<\", \"* * * * *\")\n",
    "        for i, line in enumerate(lines):\n",
    "           if any(s in line for s in strings):\n",
    "                break\n",
    "\n",
    "        if i < len(lines) - 1:\n",
    "            with open(f + '_edited', 'w') as f:\n",
    "                f.write('\\n'.join(lines[i + 1:]))\n",
    "\n",
    "\n",
    "def get_input_list(file_paths):\n",
    "    \"\"\"\n",
    "    This function takes a list of paths of files and \n",
    "    returns a 2d list that consists name of the text \n",
    "    file and the name of the author as output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get names of the text files (novels)\n",
    "    file_names=[]\n",
    "    for f in file_paths:\n",
    "        file_names.append(f.rsplit('/', 1)[1])\n",
    "\n",
    "    # Get names of the author of the novel\n",
    "    author_names=[]\n",
    "    for f in file_paths:\n",
    "        temp= f.rsplit('/', 1)[1]\n",
    "        temp_2=temp.rsplit('-', 1)[0]\n",
    "        author_names.append(temp_2.replace('-', ' '))\n",
    "    \n",
    "    # Using zip() function, aggregate \"file names\" and \"author names\" in a tuple, \n",
    "    # then convert it into a list and return\n",
    "    out_list = [list(x) for x in zip(file_names, author_names)]\n",
    "    return out_list\n",
    "    print(type(out_list))\n",
    "\n",
    "\n",
    "def read_tokenize_clean(in_file, clean=False):\n",
    "    \"\"\"\n",
    "    This function takes a txt file as an input, \n",
    "    reads the file, lowercase all words, and \n",
    "    then tokenize them using `word_tokenize()`.\n",
    "    Lastly, if argument `clean` is true it removes \n",
    "    stopwords, punctuation, and digits from tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(in_file) as reader:\n",
    "        # Read txt file and lowercase\n",
    "        text = reader.read().lower()\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        if clean:\n",
    "        # Remove stopwords, punctuation, and digits if clean=True\n",
    "            clean_tokens=[]\n",
    "            for w in tokens: \n",
    "                if w not in stops:\n",
    "                    clean_tokens.append(w)\n",
    "            return clean_tokens\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def perform_stemming(tokens, Snowball=True):\n",
    "    \"\"\"\n",
    "    This function performs stemming on tokens\n",
    "    to reduce them to their root form. Two different\n",
    "    stremmer can be selected.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Declare two different s\n",
    "    p_stemmer = PorterStemmer()\n",
    "    sb_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    ## Stemming\n",
    "    stemmed_words=[]\n",
    "    if not Snowball:\n",
    "        for w in tokens:\n",
    "            stemmed_words.append(p_stemmer.stem(w))\n",
    "        return stemmed_words\n",
    "    else:\n",
    "        for w in tokens:\n",
    "            stemmed_words.append(sb_stemmer.stem(w))\n",
    "        return stemmed_words\n",
    "\n",
    "\n",
    "def reshape_tokens(in_file, n=500, clean=False, stem=False):\n",
    "    \"\"\"\n",
    "    This function takes a text file as an input, and first,\n",
    "    reads the file, lowercase, tokenize the text. If clean=True \n",
    "    it removes stopwords, punctuation, and digits. Then,\n",
    "    it splits tokens into `n` number of chunks using list \n",
    "    comprehension and stores them in a numpy array. Finally, \n",
    "    it converts array of tokens into string, and then join them.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read, tokenize and clean the file\n",
    "    tokens = read_tokenize_clean(in_file, clean)\n",
    "\n",
    "    # Stemming the tokens\n",
    "    if stem:\n",
    "        tokens = perform_stemming(tokens)\n",
    "\n",
    "    # Split tokens list into n chunks usign list comprehension. \n",
    "    # Iterate through tokens list, add tokens to new array called \"tokens_array\" \n",
    "    # by skipping and incrementing n tokens at a time. \n",
    "    # Source: https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
    "    tokens_array = np.asarray([tokens[i:i + n] for i in range(0, len(tokens), n)])\n",
    "\n",
    "    # Get number of rows of tokens_array\n",
    "    n_rows = tokens_array.shape[0]\n",
    "\n",
    "    # Create an empty numpy array\n",
    "    out_df = np.empty(n_rows, dtype=object) \n",
    "    \n",
    "    # Convert each row of tokens_array into string using `map()`, \n",
    "    # and join all the string values\n",
    "    # Source: https://www.geeksforgeeks.org/python-program-to-convert-a-list-to-string/\n",
    "    out_df = [' '.join(map(str, tokens_array[i])) for i in range(n_rows)]\n",
    "\n",
    "    # Return the output\n",
    "    return out_df  \n",
    "\n",
    "\n",
    "def make_corpus(input_list, file_paths, n=500, clean=False, stem=False):\n",
    "    \"\"\"\n",
    "    This function takes makes corpus data to feed \n",
    "    text classifiers. Takes list of input data with\n",
    "    the file name and author name, and also takes\n",
    "    list of paths of files as an argument. It returns\n",
    "    a pandas data frame with 500 tokens in each row\n",
    "    with corresponding author in the other column.\n",
    "    \"\"\" \n",
    "    corpus = []\n",
    "\n",
    "    # Iterate through input_list and file_paths.\n",
    "    # Source: https://www.geeksforgeeks.org/python-iterate-multiple-lists-simultaneously/\n",
    "    for ([file_names, author_names], path) in zip(input_list, file_paths):\n",
    "        \n",
    "        ## Get words as sequence of n tokens\n",
    "        words = reshape_tokens(path, n, clean, stem)\n",
    "        \n",
    "        ## Create a dictionary with words and author names\n",
    "        data = {'Words':words,\n",
    "                'Author':author_names}\n",
    "        \n",
    "        ## Convert dictionary into a dataframe\n",
    "        rows_to_add = pd.DataFrame.from_dict(data)\n",
    "        \n",
    "    ## Store each data frame in a list in each iteration \n",
    "    ## and merge lists of dataframes into a single big corpus.    \n",
    "        corpus.append(rows_to_add)\n",
    "    \n",
    "    ## Return final data frame as corpus data\n",
    "    corpus = pd.concat(corpus)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "source": [
    "## Get Input Lists"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dir = Path(\"/Users/kivanc/DataMining-ML/Data/text/training_files\")\n",
    "train_files_rgx = os.path.join(training_dir, '*.txt')\n",
    "train_files_paths = glob.glob(train_files_rgx)\n",
    "\n",
    "test_dir = Path(\"/Users/kivanc/DataMining-ML/Data/text/test_files\")\n",
    "test_files_rgx = os.path.join(test_dir, '*.txt')\n",
    "test_files_paths = glob.glob(test_files_rgx)\n",
    "\n",
    "training_files = get_input_list(train_files_paths)\n",
    "test_files = get_input_list(test_files_paths)"
   ]
  },
  {
   "source": [
    "## Testing Classifiers and Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Default Settings (n=500, clean=True, stem=True)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "train_novels = make_corpus(training_files, train_files_paths, n=500, clean=True, stem=True)\n",
    "#print(train_novels)\n",
    "test_novels = make_corpus(test_files, test_files_paths, n=500, clean=True, stem=True)\n",
    "#print(test_novels)\n",
    "\n",
    "## Check the number of words per row\n",
    "#print(train_novels['Words'].str.split().str.len())\n",
    "\n",
    "# Divide data into train data, train target and test data, test target\n",
    "train_data = train_novels['Words']\n",
    "train_target = train_novels['Author']\n",
    "test_data= test_novels['Words']\n",
    "test_target = test_novels['Author']\n",
    "\n",
    "# Create a pipeline\n",
    "# Source: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#building-a-pipeline\n",
    "txt_clf_mnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb = txt_clf_mnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb.predict(test_data)\n",
    "mnb_acc = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb = txt_clf_bnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb.predict(test_data)\n",
    "bnb_acc = np.mean(predicted == test_target)\n",
    "        \n",
    "txt_clf_mnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb_tfidf = txt_clf_mnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_tfidf.predict(test_data)\n",
    "mnb_tfidf_acc = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb_tfidf = txt_clf_bnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_tfidf.predict(test_data)\n",
    "bnb_tfidf_acc = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_mnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_mnb_fpfalse = txt_clf_mnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_fpfalse.predict(test_data)\n",
    "mnb_fpfalse_acc = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_bnb_fpfalse = txt_clf_bnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_fpfalse.predict(test_data)\n",
    "bnb_fpfalse_acc = np.mean(predicted == test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Default Settings (n=500, clean=True, stem=True):\n * MultinomialNB: 0.7384441939120632, BernoulliNB: 0.6910935738444194\n\nDefault Settings with TF-IDF:\n * MultinomialNB: 0.6031567080045096, BernoulliNB: 0.6910935738444194\n\nDefault Settings with fitPrior=False:\n * MultinomialNB: 0.7406989853438557, BernoulliNB: 0.6944757609921083\n"
    }
   ],
   "source": [
    "print('Default Settings (n=500, clean=True, stem=True):', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_acc}, BernoulliNB: {bnb_acc}\")\n",
    "print()\n",
    "print('Default Settings with TF-IDF:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_tfidf_acc}, BernoulliNB: {bnb_tfidf_acc}\")\n",
    "print()\n",
    "print('Default Settings with fitPrior=False:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_fpfalse_acc}, BernoulliNB: {bnb_fpfalse_acc}\")  "
   ]
  },
  {
   "source": [
    "### No Cleanning and Stemming (n=500, clean=False, stem=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "train_novels_raw = make_corpus(training_files, train_files_paths, n=500, clean=False, stem=False)\n",
    "#print(train_novels)\n",
    "test_novels_raw = make_corpus(test_files, test_files_paths, n=500, clean=False, stem=False)\n",
    "#print(test_novels)\n",
    "\n",
    "# Divide data into train data, train target and test data, test target\n",
    "train_data = train_novels_raw['Words']\n",
    "train_target = train_novels_raw['Author']\n",
    "test_data= test_novels_raw['Words']\n",
    "test_target = test_novels_raw['Author']\n",
    "\n",
    "# Create a pipeline\n",
    "# Source: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#building-a-pipeline\n",
    "txt_clf_mnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb = txt_clf_mnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb.predict(test_data)\n",
    "mnb_acc_raw = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb = txt_clf_bnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb.predict(test_data)\n",
    "bnb_acc_raw = np.mean(predicted == test_target)\n",
    "        \n",
    "txt_clf_mnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb_tfidf = txt_clf_mnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_tfidf.predict(test_data)\n",
    "mnb_tfidf_acc_raw = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb_tfidf = txt_clf_bnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_tfidf.predict(test_data)\n",
    "bnb_tfidf_acc_raw = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_mnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_mnb_fpfalse = txt_clf_mnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_fpfalse.predict(test_data)\n",
    "mnb_fpfalse_acc_raw = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_bnb_fpfalse = txt_clf_bnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_fpfalse.predict(test_data)\n",
    "bnb_fpfalse_acc_raw = np.mean(predicted == test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No Cleanning and Stemming (n=500, clean=False, stem=False):\n * MultinomialNB: 0.7551020408163265, BernoulliNB: 0.6908909905425585\n\nNo Cleanning and Stemming with TF-IDF:\n * MultinomialNB: 0.4659034345445495, BernoulliNB: 0.6908909905425585\n\nNo Cleanning and Stemming with fitPrior=False:\n * MultinomialNB: 0.7565953210552514, BernoulliNB: 0.6958685913389746\n"
    }
   ],
   "source": [
    "print('No Cleanning and Stemming (n=500, clean=False, stem=False):', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_acc_raw }, BernoulliNB: {bnb_acc_raw }\")\n",
    "print()\n",
    "print('No Cleanning and Stemming with TF-IDF:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_tfidf_acc_raw }, BernoulliNB: {bnb_tfidf_acc_raw }\")\n",
    "print()\n",
    "print('No Cleanning and Stemming with fitPrior=False:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_fpfalse_acc_raw }, BernoulliNB: {bnb_fpfalse_acc_raw }\")  "
   ]
  },
  {
   "source": [
    "### No Cleanning and Stemming with n=50 (n=50, clean=False, stem=False)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "train_novels_raw_50 = make_corpus(training_files, train_files_paths, n=50, clean=False, stem=False)\n",
    "#print(train_novels)\n",
    "test_novels_raw_50 = make_corpus(test_files, test_files_paths, n=50, clean=False, stem=False)\n",
    "#print(test_novels)\n",
    "\n",
    "# Divide data into train data, train target and test data, test target\n",
    "train_data = train_novels_raw_50['Words']\n",
    "train_target = train_novels_raw_50['Author']\n",
    "test_data= test_novels_raw_50['Words']\n",
    "test_target = test_novels_raw_50['Author']\n",
    "\n",
    "# Create a pipeline\n",
    "# Source: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#building-a-pipeline\n",
    "txt_clf_mnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb = txt_clf_mnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb.predict(test_data)\n",
    "mnb_acc_raw_50 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb = txt_clf_bnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb.predict(test_data)\n",
    "bnb_acc_raw_50 = np.mean(predicted == test_target)\n",
    "        \n",
    "txt_clf_mnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb_tfidf = txt_clf_mnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_tfidf.predict(test_data)\n",
    "mnb_tfidf_acc_raw_50 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb_tfidf = txt_clf_bnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_tfidf.predict(test_data)\n",
    "bnb_tfidf_acc_raw_50 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_mnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_mnb_fpfalse = txt_clf_mnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_fpfalse.predict(test_data)\n",
    "mnb_fpfalse_acc_raw_50 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_bnb_fpfalse = txt_clf_bnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_fpfalse.predict(test_data)\n",
    "bnb_fpfalse_acc_raw_50 = np.mean(predicted == test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No Cleanning and Stemming with n=50 (n=50, clean=False, stem=False):\n * MultinomialNB: 0.6518119734808833, BernoulliNB: 0.6326205074522706\n\nNo Cleanning and Stemming (n=50) with TF-IDF:\n * MultinomialNB: 0.5049100244255023, BernoulliNB: 0.6326205074522706\n\nNo Cleanning and Stemming (n=50) with fitPrior=False:\n * MultinomialNB: 0.6645232042271073, BernoulliNB: 0.6474253526743432\n"
    }
   ],
   "source": [
    "print('No Cleanning and Stemming with n=50 (n=50, clean=False, stem=False):', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_acc_raw_50}, BernoulliNB: {bnb_acc_raw_50}\")\n",
    "print()\n",
    "print('No Cleanning and Stemming (n=50) with TF-IDF:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_tfidf_acc_raw_50}, BernoulliNB: {bnb_tfidf_acc_raw_50}\")\n",
    "print()\n",
    "print('No Cleanning and Stemming (n=50) with fitPrior=False:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_fpfalse_acc_raw_50}, BernoulliNB: {bnb_fpfalse_acc_raw_50}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No Cleanning and Stemming with n=250 (n=50, clean=False, stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "train_novels_raw_250 = make_corpus(training_files, train_files_paths, n=250, clean=False, stem=False)\n",
    "#print(train_novels)\n",
    "test_novels_raw_250 = make_corpus(test_files, test_files_paths, n=250, clean=False, stem=False)\n",
    "#print(test_novels)\n",
    "\n",
    "# Divide data into train data, train target and test data, test target\n",
    "train_data = train_novels_raw_250['Words']\n",
    "train_target = train_novels_raw_250['Author']\n",
    "test_data= test_novels_raw_250['Words']\n",
    "test_target = test_novels_raw_250['Author']\n",
    "\n",
    "# Building pipelines\n",
    "# Source: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#building-a-pipeline\n",
    "txt_clf_mnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb = txt_clf_mnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb.predict(test_data)\n",
    "mnb_acc_raw_250 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb = txt_clf_bnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb.predict(test_data)\n",
    "bnb_acc_raw_250 = np.mean(predicted == test_target)\n",
    "        \n",
    "txt_clf_mnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb_tfidf = txt_clf_mnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_tfidf.predict(test_data)\n",
    "mnb_tfidf_acc_raw_250 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb_tfidf = txt_clf_bnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_tfidf.predict(test_data)\n",
    "bnb_tfidf_acc_raw_250 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_mnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_mnb_fpfalse = txt_clf_mnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_fpfalse.predict(test_data)\n",
    "mnb_fpfalse_acc_raw_250 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_bnb_fpfalse = txt_clf_bnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_fpfalse.predict(test_data)\n",
    "bnb_fpfalse_acc_raw_250 = np.mean(predicted == test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No Cleanning and Stemming with n=250 (n=250, clean=False, stem=False):\n * MultinomialNB: 0.7469489414694894, BernoulliNB: 0.6946450809464508\n\nNo Cleanning and Stemming (n=250) with TF-IDF:\n * MultinomialNB: 0.5190535491905355, BernoulliNB: 0.6946450809464508\n\nNo Cleanning and Stemming (n=250) with fitPrior=False:\n * MultinomialNB: 0.750186799501868, BernoulliNB: 0.701120797011208\n"
    }
   ],
   "source": [
    "print('No Cleanning and Stemming with n=250 (n=250, clean=False, stem=False):', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_acc_raw_250}, BernoulliNB: {bnb_acc_raw_250}\")\n",
    "print()\n",
    "print('No Cleanning and Stemming (n=250) with TF-IDF:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_tfidf_acc_raw_250}, BernoulliNB: {bnb_tfidf_acc_raw_250}\")\n",
    "print()\n",
    "print('No Cleanning and Stemming (n=250) with fitPrior=False:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_fpfalse_acc_raw_250}, BernoulliNB: {bnb_fpfalse_acc_raw_250}\")  "
   ]
  },
  {
   "source": [
    "### No Cleanning and Stemming with n=1000 (n=1000, clean=False, stem=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "train_novels_raw_1000 = make_corpus(training_files, train_files_paths, n=1000, clean=False, stem=False)\n",
    "#print(train_novels)\n",
    "test_novels_raw_1000 = make_corpus(test_files, test_files_paths, n=1000, clean=False, stem=False)\n",
    "#print(test_novels)\n",
    "\n",
    "# Divide data into train data, train target and test data, test target\n",
    "train_data = train_novels_raw_1000['Words']\n",
    "train_target = train_novels_raw_1000['Author']\n",
    "test_data= test_novels_raw_1000['Words']\n",
    "test_target = test_novels_raw_1000['Author']\n",
    "\n",
    "# Create a pipeline\n",
    "# Source: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#building-a-pipeline\n",
    "txt_clf_mnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb = txt_clf_mnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb.predict(test_data)\n",
    "mnb_acc_raw_1000 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb = txt_clf_bnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb.predict(test_data)\n",
    "bnb_acc_raw_1000 = np.mean(predicted == test_target)\n",
    "        \n",
    "txt_clf_mnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb_tfidf = txt_clf_mnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_tfidf.predict(test_data)\n",
    "mnb_tfidf_acc_raw_1000 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb_tfidf = txt_clf_bnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_tfidf.predict(test_data)\n",
    "bnb_tfidf_acc_raw_1000 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_mnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_mnb_fpfalse = txt_clf_mnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_fpfalse.predict(test_data)\n",
    "mnb_fpfalse_acc_raw_1000 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_bnb_fpfalse = txt_clf_bnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_fpfalse.predict(test_data)\n",
    "bnb_fpfalse_acc_raw_1000 = np.mean(predicted == test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No Cleanning and Stemming with n=1000 (n=1000, clean=False, stem=False):\n * MultinomialNB: 0.7514910536779325, BernoulliNB: 0.6719681908548708\n\nNo Cleanning and Stemming (n=1000) with TF-IDF:\n * MultinomialNB: 0.3946322067594433, BernoulliNB: 0.6719681908548708\n\nNo Cleanning and Stemming (n=1000) with fitPrior=False:\n * MultinomialNB: 0.7514910536779325, BernoulliNB: 0.6719681908548708\n"
    }
   ],
   "source": [
    "print('No Cleanning and Stemming with n=1000 (n=1000, clean=False, stem=False):', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_acc_raw_1000}, BernoulliNB: {bnb_acc_raw_1000}\")\n",
    "print()\n",
    "print('No Cleanning and Stemming (n=1000) with TF-IDF:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_tfidf_acc_raw_1000}, BernoulliNB: {bnb_tfidf_acc_raw_1000}\")\n",
    "print()\n",
    "print('No Cleanning and Stemming (n=1000) with fitPrior=False:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_fpfalse_acc_raw_1000}, BernoulliNB: {bnb_fpfalse_acc_raw_1000}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No Cleanning and Stemming with n=5000 (n=5000, clean=False, stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "train_novels_raw_5000 = make_corpus(training_files, train_files_paths, n=5000, clean=False, stem=False)\n",
    "#print(train_novels)\n",
    "test_novels_raw_5000 = make_corpus(test_files, test_files_paths, n=5000, clean=False, stem=False)\n",
    "#print(test_novels)\n",
    "\n",
    "# Divide data into train data, train target and test data, test target\n",
    "train_data = train_novels_raw_5000['Words']\n",
    "train_target = train_novels_raw_5000['Author']\n",
    "test_data= test_novels_raw_5000['Words']\n",
    "test_target = test_novels_raw_5000['Author']\n",
    "\n",
    "# Create a pipeline\n",
    "# Source: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#building-a-pipeline\n",
    "txt_clf_mnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb = txt_clf_mnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb.predict(test_data)\n",
    "mnb_acc_raw_5000 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb = txt_clf_bnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb.predict(test_data)\n",
    "bnb_acc_raw_5000 = np.mean(predicted == test_target)\n",
    "        \n",
    "txt_clf_mnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb_tfidf = txt_clf_mnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_tfidf.predict(test_data)\n",
    "mnb_tfidf_acc_raw_5000 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb_tfidf = txt_clf_bnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_tfidf.predict(test_data)\n",
    "bnb_tfidf_acc_raw_5000 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_mnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_mnb_fpfalse = txt_clf_mnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_fpfalse.predict(test_data)\n",
    "mnb_fpfalse_acc_raw_5000 = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_bnb_fpfalse = txt_clf_bnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_fpfalse.predict(test_data)\n",
    "bnb_fpfalse_acc_raw_5000 = np.mean(predicted == test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No Cleanning and Stemming with n=5000 (n=5000, clean=False, stem=False):\n * MultinomialNB: 0.7339901477832512, BernoulliNB: 0.6305418719211823\n\nNo Cleanning and Stemming (n=5000) with TF-IDF:\n * MultinomialNB: 0.21182266009852216, BernoulliNB: 0.6305418719211823\n\nNo Cleanning and Stemming (n=5000) with fitPrior=False:\n * MultinomialNB: 0.7389162561576355, BernoulliNB: 0.6305418719211823\n"
    }
   ],
   "source": [
    "print('No Cleanning and Stemming with n=5000 (n=5000, clean=False, stem=False):', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_acc_raw_5000}, BernoulliNB: {bnb_acc_raw_5000}\")\n",
    "print()\n",
    "print('No Cleanning and Stemming (n=5000) with TF-IDF:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_tfidf_acc_raw_5000}, BernoulliNB: {bnb_tfidf_acc_raw_5000}\")\n",
    "print()\n",
    "print('No Cleanning and Stemming (n=5000) with fitPrior=False:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_fpfalse_acc_raw_5000}, BernoulliNB: {bnb_fpfalse_acc_raw_5000}\")  "
   ]
  },
  {
   "source": [
    "### Stemming but No Cleaning (n=500, clean=False, stem=True)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "train_novels_stem = make_corpus(training_files, train_files_paths, n=500, clean=False, stem=True)\n",
    "#print(train_novels)\n",
    "test_novels_stem = make_corpus(test_files, test_files_paths, n=500, clean=True, stem=True)\n",
    "#print(test_novels)\n",
    "\n",
    "# Divide data into train data, train target and test data, test target\n",
    "train_data = train_novels_stem['Words']\n",
    "train_target = train_novels_stem['Author']\n",
    "test_data= test_novels_stem['Words']\n",
    "test_target = test_novels_stem['Author']\n",
    "\n",
    "# Create a pipeline\n",
    "# Source: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#building-a-pipeline\n",
    "txt_clf_mnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb = txt_clf_mnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb.predict(test_data)\n",
    "mnb_acc_stem = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb = txt_clf_bnb.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb.predict(test_data)\n",
    "bnb_acc_stem = np.mean(predicted == test_target)\n",
    "        \n",
    "txt_clf_mnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_mnb', MultinomialNB())])\n",
    "\n",
    "txt_clf_mnb_tfidf = txt_clf_mnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_tfidf.predict(test_data)\n",
    "mnb_tfidf_acc_stem = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_tfidf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf_bnb', BernoulliNB())])\n",
    "\n",
    "txt_clf_bnb_tfidf = txt_clf_bnb_tfidf.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_tfidf.predict(test_data)\n",
    "bnb_tfidf_acc_stem = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_mnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_mnb_fpfalse = txt_clf_mnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_mnb_fpfalse.predict(test_data)\n",
    "mnb_fpfalse_acc_stem = np.mean(predicted == test_target)\n",
    "\n",
    "txt_clf_bnb_fpfalse = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf_bnb', BernoulliNB(fit_prior=False))])\n",
    "\n",
    "txt_clf_bnb_fpfalse = txt_clf_bnb_fpfalse.fit(train_data, train_target)\n",
    "predicted = txt_clf_bnb_fpfalse.predict(test_data)\n",
    "bnb_fpfalse_acc_stem = np.mean(predicted == test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Stemming but No Cleaning (n=500, clean=False, stem=True):\n * MultinomialNB: 0.7508455467869222, BernoulliNB: 0.7395715896279594\n\nStemming but No Cleaning with TF-IDF:\n * MultinomialNB: 0.6257046223224352, BernoulliNB: 0.7395715896279594\n\nStemming but No Cleaning with fitPrior=False:\n * MultinomialNB: 0.7508455467869222, BernoulliNB: 0.7395715896279594\n"
    }
   ],
   "source": [
    "print('Stemming but No Cleaning (n=500, clean=False, stem=True):', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_acc_stem}, BernoulliNB: {bnb_acc_stem}\")\n",
    "print()\n",
    "print('Stemming but No Cleaning with TF-IDF:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_tfidf_acc_stem}, BernoulliNB: {bnb_tfidf_acc_stem}\")\n",
    "print()\n",
    "print('Stemming but No Cleaning with fitPrior=False:', end='\\n * ')\n",
    "print(f\"MultinomialNB: {mnb_fpfalse_acc_stem}, BernoulliNB: {bnb_fpfalse_acc_stem}\")  "
   ]
  }
 ]
}