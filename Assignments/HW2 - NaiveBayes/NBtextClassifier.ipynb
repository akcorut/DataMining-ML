{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600265843076",
   "display_name": "Python 3.8.5 64-bit ('data_mining': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/kivanc/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/kivanc/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import string\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stops = stopwords.words('english') + list(string.punctuation) + list(string.whitespace) + list(string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = Path(\"/Users/kivanc/DataMining-ML/Data/text\")\n",
    "\n",
    "train_files_path = os.path.join(working_dir, \"training_files\", '*.txt')\n",
    "train_files_names = glob.glob(train_files_path)\n",
    "\n",
    "for f in train_files_names:\n",
    "    lines = open(f, 'r').readlines()\n",
    "\n",
    "    strings = (\"***\", \"<<<<<<<<\")\n",
    "    for i, line in enumerate(lines):\n",
    "       if any(s in line for s in strings):\n",
    "            break\n",
    "\n",
    "    if i < len(lines) - 1:\n",
    "        with open(f + '_edited', 'w') as f:\n",
    "            f.write('\\n'.join(lines[i + 1:]))\n",
    "\n",
    "\n",
    "train_files_path = os.path.join(working_dir, \"training_files\", '*.txt_edited')\n",
    "train_files_names = glob.glob(train_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['\\ufeffthe' 'project' 'gutenberg' ... 'mixture' 'notes' 'bird']\n ['burst' 'laughing' 'rose' ... 'happy' 'fond' 'one']\n ['another' 'gone' 'left' ... 'half-open' 'folding-doors' 'behold']\n ...\n ['creating' 'works' 'public' ... 'gutenberg' '”' 'associated']\n ['appearing' 'work' 'must' ... 'agreement' 'liable' 'actual']\n ['direct' 'indirect' 'consequential' ... 'status' 'compliance'\n  'particular']]\n                                                words           author\n0   copyright c 2002 david wyllie metamorphosis fr...      Kafka Franz\n1   back lifted head little could see brown belly ...      Kafka Franz\n2   thin compared size rest waved helplessly looke...      Kafka Franz\n3   walls collection textile samples lay spread ta...      Kafka Franz\n4   fur boa sat upright raising heavy fur muff cov...      Kafka Franz\n..                                                ...              ...\n83  millstone useless necklace afflictive bear yet...  Melville Herman\n84  period peeped behind screen lo bartleby button...  Melville Herman\n85  cane going door tranquilly turned added -- '' ...  Melville Herman\n86  bullying bravado sort choleric hectoring strid...  Melville Herman\n87  prefer man preferences assumptions breakfast w...  Melville Herman\n\n[1861 rows x 2 columns]\n"
    }
   ],
   "source": [
    "def get_input_list(file_names):\n",
    "    fnames=[]\n",
    "    for f in file_names:\n",
    "        fnames.append(f.rsplit('/', 1)[1])\n",
    "\n",
    "    author=[]\n",
    "    for f in file_names:\n",
    "        temp= f.rsplit('/', 1)[1]\n",
    "        temp_2=temp.rsplit('-', 1)[0]\n",
    "        author.append(temp_2.replace('-', ' '))\n",
    "    \n",
    "    out_list = [list(x) for x in zip(fnames, author)]\n",
    "    return out_list\n",
    "\n",
    "\n",
    "def read_tokenize_clean(file, clean=True, tokenize=True):\n",
    "    with open(file) as reader:\n",
    "        text = reader.read().lower()\n",
    "        if tokenize:\n",
    "            tokens= word_tokenize(text)\n",
    "            if clean:\n",
    "                clean_tokens=[]\n",
    "                for w in tokens: \n",
    "                    if w not in stops:\n",
    "                        clean_tokens.append(w)\n",
    "                return clean_tokens\n",
    "            return tokens\n",
    "        return text\n",
    "\n",
    "\n",
    "def reshape_tokens(file, n=500):\n",
    "    # read, tokenize and clean the file\n",
    "    tokens= read_tokenize_clean(file, clean=True, tokenize=True)\n",
    "\n",
    "    # split tokens into n chunks\n",
    "    tokens_array = np.array_split(np.array(tokens),n)\n",
    "    \n",
    "    out_df = np.empty(len(tokens)//n, dtype=object) \n",
    "    for i in range(len(tokens)//n):\n",
    "        out_df[i] = ' '.join(map(str, tokens_array[i])) \n",
    "    return out_df\n",
    "\n",
    "\n",
    "def make_corpus(input_list, n=500):\n",
    "    out_df=[]\n",
    "    for [fnames,author] in input_list:\n",
    "        input_file = os.path.join(working_dir, fnames)\n",
    "        words = reshape_tokens(input_file, n)\n",
    "        df = pd.DataFrame(words, columns=['words'])\n",
    "        df['author'] = author\n",
    "        out_df.append(df)\n",
    "    corpus_data = pd.concat(out_df)\n",
    "    return corpus_data\n",
    "\n",
    "training_dir = Path(\"/Users/kivanc/DataMining-ML/Data/text/training_files\")\n",
    "train_files_names = glob.glob(training_dir)\n",
    "\n",
    "test_dir = Path(\"/Users/kivanc/DataMining-ML/Data/text/test_files\")\n",
    "test_files_names = glob.glob(test_dir)\n",
    "\n",
    "training_files = getInputList(train_files_names)\n",
    "test_files = etInputList(test_files_names)\n",
    "\n",
    "training_df = make_corpus(training_files, n=500)\n",
    "print(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "N=500 MultinomialNB: 0.5572519083969466; BernoulliNB: 0.5343511450381679\n['Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Nathaniel Hawthorne' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Nathaniel Hawthorne' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Herman Melville' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Herman Melville' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Herman Melville' 'Herman Melville'\n 'Herman Melville' 'Nathaniel Hawthorne' 'Nathaniel Hawthorne'\n 'Nathaniel Hawthorne' 'Herman Melville' 'Nathaniel Hawthorne']\n"
    }
   ],
   "source": [
    "def naive_bayes(n = 500):\n",
    "    train_df = process_files(train_files, n = n)\n",
    "    test_df = process_files(test_files, n = n)\n",
    "    \n",
    "    X_train= train_df['text']\n",
    "    targets_train = train_df['author']\n",
    "    X_test= test_df['text']\n",
    "    targets_test = test_df['author']\n",
    "    \n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_train)\n",
    "    X_new_counts = count_vect.transform(X_test)\n",
    "\n",
    "    clfMNB = MultinomialNB().fit(X_train_counts, targets_train)\n",
    "    predicted = clfMNB.predict(X_new_counts)    \n",
    "    mnb = np.mean(predicted == targets_test)\n",
    "\n",
    "    clfBNB = BernoulliNB().fit(X_train_counts, targets_train)\n",
    "    predicted = clfBNB.predict(X_new_counts)    \n",
    "    bnb = np.mean(predicted == targets_test)\n",
    "    \n",
    "    print(f\"N={n} MultinomialNB: {mnb}; BernoulliNB: {bnb}\")\n",
    "    print(predicted)\n",
    "\n",
    "naive_bayes()"
   ]
  }
 ]
}