{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following discusses a few points from Chapter 5 of Witten et al.'s **Data Mining: Practical Machine Learning Tools and Techniques** (4th ed).* Specifically, these notes discuss defining a confidence interval based on a classifier's performance on a dataset available (usually, data is scarce). They also discuss using statistical tests to evaluate the relative performance of classifiers. \n",
    "\n",
    "\n",
    "## Estimating classifier performance\n",
    "\n",
    "\n",
    "When we develop a classification model, its performance (e.g., accuracy) will vary from data set to data set (even if the data sets were taken from the same population), and in general we'd like a way of estimating how well the model is expected to perform. One thing we can do is use the results obtained from a test set and define a confidence interval based on that.  \n",
    "\n",
    "The idea is that applying the classifier to randomly selected data sets is a Bernouli process, like flipping a coin. If the probability of getting heads is $p$, then for any sequence of $N$ coin flips, the actual frequency of heads will be somewhere around $p$. If we were to record the values for multiple sequences of coin flips,  it turns out that the variance  of the values will be $p(1-p)/N$. For large $N$, the distribution will be approximately normal. \n",
    "\n",
    "Suppose a classifier on a given set yields accuracy f=75%. To standardize $f$ (yielding a $z$ score), we would use the following. \n",
    "\n",
    "$\\frac{f - p}{\\sqrt{p(1-p)/N}}$\n",
    "\n",
    "To define an interval, we would need to choose a suitable value for $z$ (based upon whatever confidence level we are interested in, and then compute:\n",
    "\n",
    "$P(-z < \\frac{f - p}{\\sqrt{p(1-p)/N}} < +z)$\n",
    "\n",
    "Actually, what we are interested in is solving for $p$ (we treat the above as an equality and then solve for $p$). The details are omitted, but we are left with.  \n",
    "\n",
    "$p = (f + \\frac{z^2}{2N} \\pm z\\sqrt{\\frac{f}{N}-\\frac{f^2}{N}+\\frac{z^2}{4N^2}})/(1+\\frac{z^2}{N})$\n",
    "\n",
    "A Python function computing confidence intervals based on this is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import norm,t, binom\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np;\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdbc_data = '../data-sets/wdbc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines interval level based on the above discussion. \n",
    "def confidence(f, N, c=0.9):\n",
    "    z = norm.ppf(1-((1-c)/2))\n",
    "    t1 = z*math.sqrt(f/N - (f*f)/N + (z*z)/(4*N*N))\n",
    "    t2 = f + (z*z)/(2*N) \n",
    "    t3 = (1+(z*z)/N)\n",
    "    return [(t2-t1)/t3, (t2+t1)/t3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t0.0000\n",
      "0.05\t0.0654\n",
      "0.10\t0.1310\n",
      "0.16\t0.1972\n",
      "0.21\t0.2643\n",
      "0.26\t0.3326\n",
      "0.31\t0.4024\n",
      "0.36\t0.4743\n",
      "0.42\t0.5488\n",
      "0.47\t0.6264\n",
      "0.52\t0.7080\n",
      "0.57\t0.7946\n",
      "0.63\t0.8876\n",
      "0.68\t0.9891\n",
      "0.73\t1.1019\n",
      "0.78\t1.2307\n",
      "0.83\t1.3841\n",
      "0.89\t1.5795\n",
      "0.94\t1.8655\n",
      "0.99\t2.5758\n"
     ]
    }
   ],
   "source": [
    "# Aside: we'll print out some z scores for some confidence levels. \n",
    "dist = norm(0,1)\n",
    "x = np.linspace(0.0,0.99,20)\n",
    "for i in x:\n",
    "    half = i+(1-i)/2\n",
    "    print(\"{0:.2f}\\t{1:.4f}\".format(i, dist.ppf(half)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compare the manual calculations to what we get using a normal distribution from `scipy.stats`. Note that the use of a normal distribution will only be appropriate for a large value for $n$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7320513138468852, 0.7671288454309665]\n",
      "(0.7324516324736291, 0.7675483675263709)\n"
     ]
    }
   ],
   "source": [
    "dist = norm(0,1)\n",
    "z = norm.ppf(0.9)\n",
    "\n",
    "f = 0.75\n",
    "c = 0.8\n",
    "n = 1000\n",
    "\n",
    "# compare the manual calculation to using norm.interval.\n",
    "print(confidence(f,n,c))\n",
    "print(norm.interval(c, loc=f, scale=math.sqrt(f*(1-f))/math.sqrt(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6907697268228327, 0.8011510915140075]\n",
      "(0.6945071893989329, 0.8054928106010671)\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "p = 0.75\n",
    "print(confidence(p,n,c))\n",
    "print(norm.interval(c, loc=p, scale=math.sqrt(p*(1-p))/math.sqrt(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5483172316247326, 0.881148427329997]\n",
      "(0.5745163247362906, 0.9254836752637094)\n"
     ]
    }
   ],
   "source": [
    "p = 0.75\n",
    "c = 0.8\n",
    "n = 10\n",
    "\n",
    "print(confidence(p,n,c))\n",
    "print(norm.interval(c, loc=p, scale=math.sqrt(p*(1-p))/math.sqrt(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using t-tests to compare classifiers\n",
    "\n",
    "It's possible to use t-tests to compare 2 or more classifiers. Importantly, one should not use a standard paired t-test, as the this tends to falsely indicate that 2 models are different when they are not. Witten et al. instead recommend what they call the \\emph{corrected resampled t-test}. They indicate that it is acceptable to use this test when compairing models on resampled data (multiple pairs of accuracy scores are obtained by repeatedly generating training test splits over the data), or else when cross validation is used. \n",
    "\n",
    "Below, we'll use the Wisconsin breast cancer data set and compare decision trees to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:\t (569, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radiusAvg</th>\n",
       "      <th>textureAvg</th>\n",
       "      <th>perimeterAvg</th>\n",
       "      <th>areaAvg</th>\n",
       "      <th>smoothnessAvg</th>\n",
       "      <th>compactnessAvg</th>\n",
       "      <th>concavityAvg</th>\n",
       "      <th>concavepointsAvg</th>\n",
       "      <th>symmetryAvg</th>\n",
       "      <th>...</th>\n",
       "      <th>textureWorst</th>\n",
       "      <th>perimeterWorst</th>\n",
       "      <th>areaWorst</th>\n",
       "      <th>smoothnessWorst</th>\n",
       "      <th>compactnessWorst</th>\n",
       "      <th>concavityWorst</th>\n",
       "      <th>concavepointsWorst</th>\n",
       "      <th>symmetryWorst</th>\n",
       "      <th>fractaldimWorst</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  radiusAvg  textureAvg  perimeterAvg  areaAvg  smoothnessAvg  \\\n",
       "0    842302      17.99       10.38        122.80   1001.0        0.11840   \n",
       "1    842517      20.57       17.77        132.90   1326.0        0.08474   \n",
       "2  84300903      19.69       21.25        130.00   1203.0        0.10960   \n",
       "3  84348301      11.42       20.38         77.58    386.1        0.14250   \n",
       "4  84358402      20.29       14.34        135.10   1297.0        0.10030   \n",
       "\n",
       "   compactnessAvg  concavityAvg  concavepointsAvg  symmetryAvg  ...  \\\n",
       "0         0.27760        0.3001           0.14710       0.2419  ...   \n",
       "1         0.07864        0.0869           0.07017       0.1812  ...   \n",
       "2         0.15990        0.1974           0.12790       0.2069  ...   \n",
       "3         0.28390        0.2414           0.10520       0.2597  ...   \n",
       "4         0.13280        0.1980           0.10430       0.1809  ...   \n",
       "\n",
       "   textureWorst  perimeterWorst  areaWorst  smoothnessWorst  compactnessWorst  \\\n",
       "0         17.33          184.60     2019.0           0.1622            0.6656   \n",
       "1         23.41          158.80     1956.0           0.1238            0.1866   \n",
       "2         25.53          152.50     1709.0           0.1444            0.4245   \n",
       "3         26.50           98.87      567.7           0.2098            0.8663   \n",
       "4         16.67          152.20     1575.0           0.1374            0.2050   \n",
       "\n",
       "   concavityWorst  concavepointsWorst  symmetryWorst  fractaldimWorst  c  \n",
       "0          0.7119              0.2654         0.4601          0.11890  0  \n",
       "1          0.2416              0.1860         0.2750          0.08902  0  \n",
       "2          0.4504              0.2430         0.3613          0.08758  0  \n",
       "3          0.6869              0.2575         0.6638          0.17300  0  \n",
       "4          0.4000              0.1625         0.2364          0.07678  0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll read in the data set using Pandas. \n",
    "df = pd.read_csv(wdbc_data, header=0)\n",
    "print(\"shape:\\t\",df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create the classifiers and store the results of multiple stratified k-fold cross-validation runs. If there are $n$ runs and $k$ folds, we will have $n*k$ pairs of numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Tree=0.923+/-0.036 Reg=0.952+/-0.030\n",
      "\n",
      "t-value -7.690228626789212, p-value 1.0992789213902389e-11\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree, linear_model\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import ttest_rel, ttest_ind\n",
    "data = df.iloc[:].values\n",
    "\n",
    "# extract X and y values from the data set; \n",
    "X = data[:, 1:-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# create the classifier objects; \n",
    "dtree = tree.DecisionTreeClassifier()\n",
    "logr = linear_model.LogisticRegression(solver='liblinear')\n",
    "\n",
    "repeats = 10;\n",
    "cv = 10; \n",
    "scores = [] # to store scores of each run+fold. \n",
    "\n",
    "# for run and fold, fit the models and store performance on the test set. \n",
    "for i in range(repeats):\n",
    "    k_fold = StratifiedKFold(n_splits=cv, shuffle=True)\n",
    "    for train_indices, test_indices in k_fold.split(X,y):\n",
    "        dtree.fit(X[train_indices], y[train_indices]) \n",
    "        logr.fit(X[train_indices],    y[train_indices])     \n",
    "        predictedTree = dtree.predict(X[test_indices])\n",
    "        predictedLogR = logr.predict(X[test_indices])\n",
    "        scores.append(\n",
    "        [metrics.accuracy_score(y[test_indices], predictedTree), metrics.accuracy_score(y[test_indices], predictedLogR\n",
    "                                   )])\n",
    "scores = np.array(scores);\n",
    "\n",
    "# print out average accuracy over all of the runs and folds for each classifier.\")\n",
    "print(\"Accuracy: Tree={:.3f}+/-{:.3f} Reg={:.3f}+/-{:.3f}\".format(np.mean(scores[:,0]),np.std(scores[:,0]),np.mean(scores[:,1]),np.std(scores[:,1]),))\n",
    "\n",
    "# print out the results of simple paried t-test\n",
    "(tval, pval) = ttest_rel(scores[:,0], scores[:,1])\n",
    "print(\"\\nt-value {}, p-value {}\".format(tval, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, following the textbook's discussion, we manually calculate the t-score based on our recorded accuracy scores and then the p-value.The results should be the same as using scipy's builtin  `ttest_rel`.\n",
    "\n",
    "The t-statistic is calculated as  $\\frac{\\overline{d}}{\\sqrt{\\frac{1}{k}\\sigma_d^2}}$, where  $\\overline{d}$ is the mean of the differences between the paired accuracy values and $k$ is the degrees of freedom, which is the number of pairs - 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "t-value -7.690228626789212, p-value 1.0992789213902389e-11\n"
     ]
    }
   ],
   "source": [
    "# t-statistic based on simple differences of the two data sets. \n",
    "diff = scores[:,0]-scores[:,1]\n",
    "variance = np.var(diff)\n",
    "k = len(diff)-1 # note the text doesn't clearly indicate subtracting the 1 to obtain the degrees of freedom. \n",
    "tval = np.mean(diff)/math.sqrt(variance/k)\n",
    "dist = t(k)\n",
    "pval = dist.cdf(tval)*2\n",
    "print(\"\\nt-value {}, p-value {}\".format(tval, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above very likely indicate that the results of two classifiers are statistically significant (*note that when the jupyter notebook scripts are rerun, it is possible that the results will change*). However, the assumptions of the t-test are violated (for one, we're resampling the data set multiple times, and so the samples are not indpendent of each other). As such, the results of the t-test are not to be trusted. \n",
    "\n",
    "Witten et al. recommend using this corrected version of the statistic: $\\frac{\\overline{d}}{\\sqrt{(\\frac{1}{k}+\\frac{n_2}{n_1})\\sigma_d^2}}$. Here, $n_2$ and $n_1$ refer to the size of the test set and the training set (in this case, the ratio would be 1/9). We could do the same thing if we use the repeated holdout method rather than repeated cross-validation. Witten et al. indicate that the process and interpretation would be the same. \n",
    "\n",
    "The results of this corrected test could easily could be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "t-value -2.2380467135509714, p-value 0.027458518568216482\n"
     ]
    }
   ],
   "source": [
    "# corrected resampled t-statistic (assumes we use repeated train/test splits or repeated cross-validation). \n",
    "diff = scores[:,0]-scores[:,1]\n",
    "variance = np.var(diff)\n",
    "foldcount = 10;\n",
    "test_size = math.floor(data.shape[0]/foldcount);\n",
    "train_size = math.floor(data.shape[0]-test_size);\n",
    "tval = np.mean(diff)/math.sqrt(variance*(1/k+ test_size/train_size))\n",
    "pval = dist.cdf(tval)*2\n",
    "print(\"\\nt-value {}, p-value {}\".format(tval, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, rather than use cross-validation, we just use repeated training/test splits. We again compare the results of the plain paired t-test vs the \"corrected resampled t-test\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree=0.929+/-0.023 Reg=0.953+/-0.019\n",
      "\n",
      "t-value -9.676954526642609, p-value 5.544164615402578e-16\n"
     ]
    }
   ],
   "source": [
    "# uncorrected paired t-test using repeated holdout method. \n",
    "from sklearn.model_selection import train_test_split\n",
    "test_percent = 0.20\n",
    "scores = []\n",
    "k = 100;\n",
    "for i in range(k):\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=test_percent, shuffle=True)\n",
    "    dtree.fit(X_train, y_train) \n",
    "    logr.fit(X_train,  y_train)     \n",
    "    predictedTree = dtree.predict(X_test)\n",
    "    predictedLogR = logr.predict(X_test)\n",
    "    scores.append(\n",
    "        [metrics.accuracy_score(y_test, predictedTree), metrics.accuracy_score(y_test, predictedLogR\n",
    "                                   )])\n",
    "scores = np.array(scores);\n",
    "\n",
    "# output results of simple paired t-test. \n",
    "print(\"Tree={:.3f}+/-{:.3f} Reg={:.3f}+/-{:.3f}\".format(np.mean(scores[:,0]),np.std(scores[:,0]),np.mean(scores[:,1]),np.std(scores[:,1]),))\n",
    "(tval, pval) = ttest_rel(scores[:,0], scores[:,1])\n",
    "print(\"\\nt-value {}, p-value {}\".format(tval, pval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 100\n",
      "\n",
      "t-value -1.9154628598235999, p-value 0.0583191760320691\n"
     ]
    }
   ],
   "source": [
    "# corrected paired t-test using repeated holdout method\n",
    "diff = scores[:,0]-scores[:,1]\n",
    "variance = np.var(diff)\n",
    "print(\"k:\",k)\n",
    "test_size = math.floor(data.shape[0]*test_percent);\n",
    "train_size = math.floor(data.shape[0]-test_size);\n",
    "\n",
    "tval = np.mean(diff)/math.sqrt(variance*(1/k+ test_size/train_size))\n",
    "pval = dist.cdf(tval)*2\n",
    "print(\"\\nt-value {}, p-value {}\".format(tval, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question that you might have is this: Looking at how the t-statistic is computed ($\\frac{\\overline{d}}{\\sqrt{\\sigma_d^2/k}})$, won't the magnitude of the t-statistic be very large in realistic data mining scenarios?\n",
    "\n",
    "Yes, that appears to be the case. Below we compare the plots of the uncorrected t-statistic and the corrected version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2453acbbe08>,\n",
       " <matplotlib.lines.Line2D at 0x2453b3c6888>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tvals = []\n",
    "for k in range(1,200):\n",
    "    tvals.append( [k, \n",
    "                   np.absolute(np.mean(diff)/math.sqrt(variance*(1/k+ test_size/train_size))),\n",
    "                   np.absolute(np.mean(diff)/math.sqrt(variance*(1/k)))])\n",
    "pltdata = np.array(tvals);\n",
    "plt.plot(pltdata[:,0],pltdata[:,1],\"r\", pltdata[:,0],pltdata[:,2],\"b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
