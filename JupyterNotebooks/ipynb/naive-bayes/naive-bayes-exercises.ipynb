{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Exercises\n",
    "\n",
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "## Function definitions\n",
    "\n",
    "We'll define some useful routines for generating nonsense sentences of words. These routines will be used to generate a corpus of documents to use with Naive Bayes classifiers. \n",
    "\n",
    "* `distribution`: takes a list of words and weights and generates a distribution based on the weights (this just scales the total weight to 1). If no weights are provided, a random distribution is created. \n",
    "* `sentence`: takes the word vocabulary, a distribution, and a sentence size, and returns a sentence -- a string of tokens where each token is selected based on the provided distribution.  \n",
    "* `sentences`: generates a collection of random sentences based on the provided words, distribution, and sentence size. \n",
    "* `random_choice`: takes a list of words and a distribution and returns a single word based on the distribution.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def distribution(words, probs=None):\n",
    "    \"\"\"Given a list (and weights for them), return a probability \n",
    "    distribution for the words.\n",
    "\n",
    "    This is used to generate sentences for different classes.\"\"\"\n",
    "    if probs is None:\n",
    "        probs = np.random.random(len(words))\n",
    "    else:\n",
    "        probs = np.array(probs)\n",
    "    return probs/probs.sum()\n",
    "    \n",
    "def sentence(words, dist, size):\n",
    "    \"\"\"Given a list of words and a probability distribution for the words,\n",
    "    generate a sentence of a given number of tokens.\n",
    "    \"\"\"\n",
    "    return ' '.join([random_choice(words,dist) for i in range(size)])\n",
    "\n",
    "def sentences(words, dist, size, howmany, class_label):\n",
    "    df = pd.DataFrame([sentence(words, dist, size) for i in range(howmany)], columns = ['X'])\n",
    "    df['target'] = class_label\n",
    "    return df\n",
    "\n",
    "def random_choice(words, dist):\n",
    "    \"\"\"Given a list of words and a probability distribution for the words,\n",
    "    return a single word.\"\"\"\n",
    "    return np.random.choice(words, p=dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating conditional probabilities for classes\n",
    "\n",
    "Now that we have the definitions, we'll generate probabilities for a small vocabulary and two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Probability, by Class\n",
      "Class 1\n",
      "\tone\t0.3571\n",
      "\ttwo\t0.3571\n",
      "\tred\t0.0714\n",
      "\tblue\t0.0714\n",
      "\tfish\t0.0714\n",
      "\tbird\t0.0714\n",
      "Class 2\n",
      "\tone\t0.0714\n",
      "\ttwo\t0.0714\n",
      "\tred\t0.0714\n",
      "\tblue\t0.0714\n",
      "\tfish\t0.3571\n",
      "\tbird\t0.3571\n"
     ]
    }
   ],
   "source": [
    "# The vocabulary to use. Change this to suit your needs.\n",
    "vocabulary = ['one', 'two', 'red', 'blue', 'fish', 'bird']\n",
    "\n",
    "# The relative weights of the words in each class. \n",
    "# These must have the same size as the vocabulary size. \n",
    "weights1 =     [5,5,1,1,1,1]\n",
    "weights2 =     [1,1,1,1,5,5]\n",
    "\n",
    "d1 = distribution(vocabulary, probs=weights1)\n",
    "d2 = distribution(vocabulary, probs=weights2)\n",
    "\n",
    "\n",
    "print(\"Word Probability, by Class\")\n",
    "print(\"Class 1\")\n",
    "for word, prob in zip(vocabulary,d1):\n",
    "    print(f'\\t{word}\\t{prob:.4f}')\n",
    "\n",
    "    \n",
    "print(\"Class 2\")\n",
    "for word, prob in zip(vocabulary,d2):\n",
    "    print(f'\\t{word}\\t{prob:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a corpus\n",
    "\n",
    "We'll take the probabilities and generate a set of sentences (a corpus), printing the corpus out afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          X  target\n",
      "0      two two two blue red       1\n",
      "1      blue one two one two       1\n",
      "2     two bird two blue two       1\n",
      "3     one one blue red fish       1\n",
      "4     red blue fish two two       1\n",
      "5    red two bird bird bird       2\n",
      "6    red fish fish blue red       2\n",
      "7  bird fish fish bird bird       2\n",
      "8   red bird fish fish bird       2\n",
      "9    fish fish bird two two       2\n"
     ]
    }
   ],
   "source": [
    "# number of sentences of each class to generate\n",
    "class_1_size = 5\n",
    "class_2_size = 5\n",
    "\n",
    "# number of tokens in each sentence\n",
    "sentence_size = 5\n",
    "\n",
    "# create dataframes of sentences for each class\n",
    "df1 = sentences(vocabulary, d1, sentence_size, class_1_size, 1)\n",
    "df2 = sentences(vocabulary, d2, sentence_size, class_2_size, 2)\n",
    "\n",
    "# join them into a single dataframe (the corpus)\n",
    "corpus = df1.append(df2, ignore_index=True)\n",
    "\n",
    "# assign the targets column to a variable. \n",
    "targets = corpus.target\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the Corpus (counting words)\n",
    "\n",
    "To run a Naive Bayes classifier, we need to convert each string in the corpus into vectors of word counts. We use `CountVectorizer` to do this.  \n",
    "\n",
    "Afterwards, we print out the new form of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD COUNTS\n",
      "--------------------\n",
      "   bird  blue  fish  one  red  two\n",
      "0     0     1     0    0    1    3\n",
      "1     0     1     0    2    0    2\n",
      "2     1     1     0    0    0    3\n",
      "3     0     1     1    2    1    0\n",
      "4     0     1     1    0    1    2\n",
      "5     3     0     0    0    1    1\n",
      "6     0     1     2    0    2    0\n",
      "7     3     0     2    0    0    0\n",
      "8     2     0     2    0    1    0\n",
      "9     1     0     2    0    0    2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(corpus.X)\n",
    "names = count_vect.get_feature_names()\n",
    "arr = X_train_counts.toarray()\n",
    "counts_df = pd.DataFrame(arr, columns=names)\n",
    "\n",
    "print('WORD COUNTS')\n",
    "print(f'{\"-\"*20}')\n",
    "print(counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the corpus by class\n",
    "\n",
    "We do this so that we can easily compute counts and other statistics for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD COUNTS\n",
      "---------------Class 1---------------\n",
      "   bird  blue  fish  one  red  two\n",
      "0     0     0     0    1    0    4\n",
      "1     1     1     0    1    0    2\n",
      "2     0     1     0    2    0    2\n",
      "3     0     2     1    2    0    0\n",
      "4     0     2     0    0    0    3\n",
      "---------------Class 2---------------\n",
      "   bird  blue  fish  one  red  two\n",
      "5     2     0     1    0    1    1\n",
      "6     3     1     1    0    0    0\n",
      "7     4     0     0    0    1    0\n",
      "8     3     1     1    0    0    0\n",
      "9     2     1     0    2    0    0\n"
     ]
    }
   ],
   "source": [
    "print('WORD COUNTS')\n",
    "print(f'{\"-\"*15}Class 1{\"-\"*15}')\n",
    "class_1_counts = counts_df[targets == 1]\n",
    "print(class_1_counts)\n",
    "print(f'{\"-\"*15}Class 2{\"-\"*15}')\n",
    "class_2_counts = counts_df[targets == 2]\n",
    "print(class_2_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing conditional probabilities\n",
    "\n",
    "Though we already used probabilities that we defined ourselfs to generate the corpus, we now use the actual word counts for the corpus to compute conditional probabilities empirically. This is because in the real world, the corpus would not be artificially generated. \n",
    "\n",
    "First we count tokens of each word for each class, taking **add-1** smoothing into consideration.  \n",
    "\n",
    "### Counting the tokens for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 word counts, no +1 smoothing. Total count is 25\n",
      "   bird  blue  fish  one  red  two\n",
      "0     1     6     1    6    0   11\n",
      "\n",
      "Class 1 word counts, with +1 smoothing. Total count is 31\n",
      "   bird  blue  fish  one  red  two\n",
      "0     2     7     2    7    1   12\n"
     ]
    }
   ],
   "source": [
    "# count the total number of words in all sentences of class 1.\n",
    "total_tokens_class_1 = class_1_counts.values.sum()\n",
    "\n",
    "# then compute the total count of each individual word.\n",
    "class_1_word_counts = pd.DataFrame(class_1_counts.sum(axis=0))\n",
    "\n",
    "# add 1 to each of these totals.\n",
    "class_1_word_counts_smoothed = class_1_word_counts+1\n",
    "\n",
    "print(f'Class 1 word counts, no +1 smoothing. Total count is {total_tokens_class_1}')\n",
    "print(class_1_word_counts.T)\n",
    "\n",
    "total_tokens_class_1 = class_1_word_counts_smoothed.values.sum()\n",
    "\n",
    "print(f'\\nClass 1 word counts, with +1 smoothing. Total count is {total_tokens_class_1}')\n",
    "print(class_1_word_counts_smoothed.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same thing for class 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 2 word counts, no +1 smoothing. Total count is 25\n",
      "   bird  blue  fish  one  red  two\n",
      "0    14     3     3    2    2    1\n",
      "\n",
      "Class 2 word counts, with +1 smoothing. Total count is 31\n",
      "   bird  blue  fish  one  red  two\n",
      "0    15     4     4    3    3    2\n"
     ]
    }
   ],
   "source": [
    "total_tokens_class_2 = class_2_counts.values.sum()\n",
    "class_2_word_counts = pd.DataFrame(class_2_counts.sum(axis=0))\n",
    "class_2_word_counts_smoothed = class_2_word_counts+1\n",
    "\n",
    "print(f'Class 2 word counts, no +1 smoothing. Total count is {total_tokens_class_2}')\n",
    "print(class_2_word_counts.T)\n",
    "\n",
    "total_tokens_class_2 = class_2_word_counts_smoothed.values.sum()\n",
    "\n",
    "print(f'\\nClass 2 word counts, with +1 smoothing. Total count is {total_tokens_class_2}')\n",
    "print(class_2_word_counts_smoothed.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the probabilities\n",
    "\n",
    "We then use the counts to compute conditional probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Probabilities, Class 1\n",
      "       bird      blue      fish       one       red       two\n",
      "0  0.064516  0.225806  0.064516  0.225806  0.032258  0.387097\n"
     ]
    }
   ],
   "source": [
    "cond_prob_class_1 = class_1_word_counts_smoothed/total_tokens_class_1\n",
    "\n",
    "print('Conditional Probabilities, Class 1')\n",
    "print(cond_prob_class_1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Probabilities, Class 2\n",
      "       bird      blue      fish       one       red       two\n",
      "0  0.483871  0.129032  0.129032  0.096774  0.096774  0.064516\n"
     ]
    }
   ],
   "source": [
    "cond_prob_class_2 = class_2_word_counts_smoothed/total_tokens_class_2\n",
    "\n",
    "print('Conditional Probabilities, Class 2')\n",
    "print(cond_prob_class_2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute the class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability, Class 1:\t0.5\n",
      "Probability, Class 2:\t0.5\n"
     ]
    }
   ],
   "source": [
    "class_1_size, _ = class_1_counts.shape\n",
    "class_2_size, _ = class_2_counts.shape\n",
    "\n",
    "class_1_prob = class_1_size/(class_1_size+class_2_size) \n",
    "class_2_prob = class_2_size/(class_1_size+class_2_size)\n",
    "\n",
    "print(f'Probability, Class 1:\\t{class_1_prob}')\n",
    "print(f'Probability, Class 2:\\t{class_2_prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying a new example\n",
    "\n",
    "At this point, we have everything we need to classify a new instance. Suppose the instance is the one below.  As with the original data, we must vectorize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = ['bird']\n",
    "X_new_counts = count_vect.transform(new_data)\n",
    "arr = X_new_counts.toarray()\n",
    "new_counts_df = pd.DataFrame(arr, columns=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we can compute the probabilities needed for classification. \n",
    "\n",
    "The below code will compute the following products for Class 1 and Class 2. $word_i$ refers to a particular word, and $count_i$ indicates the number of times it occurs in the sentences we are attempting to classify. $class_j$ indicates the class. \n",
    "\n",
    "$p(word_i|class_j)^{count_i}\\times \\ldots \\times p(word_k|class_j)^{count_k}\\times p(class_j)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 Result:\t0.03225806451612903\n",
      "Class 2 Result:\t0.24193548387096775\n"
     ]
    }
   ],
   "source": [
    "class_1_probs = np.power(cond_prob_class_1.T,new_counts_df)\n",
    "result_1 = np.product(class_1_probs.values)*class_1_prob\n",
    "class_2_probs = np.power(cond_prob_class_2.T,new_counts_df)\n",
    "result_2 = np.product(class_2_probs.values)*class_2_prob\n",
    "\n",
    "print(f'Class 1 Result:\\t{result_1}')\n",
    "print(f'Class 2 Result:\\t{result_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on which value is larger, we classify the new instance as belonging to the corresponding class. We can also combine the two results and compute the probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities\n",
      "[0.11764705882352942, 0.8823529411764707]\n"
     ]
    }
   ],
   "source": [
    "print('Probabilities')\n",
    "print([result_1/(result_1+result_2),result_2/(result_1+result_2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to scikit learn. \n",
    "\n",
    "Once we have the vectorized input (using the count of each word, each document can be viewed as a vector), we can create a `scikit-learn` classifier for it. We'll use `MultinomialNB`, one of scikit-learn's defined Naive Bayes classifiers. It's appropriate for working with text. \n",
    "\n",
    "After we create the classifier, we first fit it using the corpus, and then invoke `predict` to classify the new instance. \n",
    "\n",
    "At the end, we also print out the probabilities, which should match our hand-rolled calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions\n",
      "bird => 2\n",
      "\n",
      "Probabilities\n",
      "[[0.11764706 0.88235294]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# create the classifier instance and then fit it to the corpus. \n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts, targets)\n",
    "\n",
    "# then classify the new instance.\n",
    "X_new_counts = count_vect.transform(new_data)\n",
    "predicted = clf.predict(X_new_counts)\n",
    "\n",
    "# zip iterates through new_data and predicted in parallel, forming 2-tuples.\n",
    "print('Predictions')\n",
    "for doc, category in zip(new_data, predicted):\n",
    "    print(f'{doc} => {category}')\n",
    "\n",
    "print('\\nProbabilities')\n",
    "print(clf.predict_proba(X_new_counts))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using logs\n",
    "\n",
    "Recall that computing the product of many values close to 0 could result in a floating point underflow error. To get around this, it is common practice to instead compute the some of the logarithms of the probabilities. This is shown below. At the end, we again compute the probabilities for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities Class 1\n",
      "       bird      blue      fish       one       red       two\n",
      "0 -3.954196 -2.146841 -3.954196 -2.146841 -4.954196 -1.369234\n",
      "Log probabilities Class 2\n",
      "       bird      blue      fish       one       red       two\n",
      "0 -1.047306 -2.954196 -2.954196 -3.369234 -3.369234 -3.954196\n",
      "\n",
      "Result, Class 1: -4.954196310386875\n",
      "Result, Class 2: -2.0473057147783567\n",
      "[0.11764705882352942, 0.8823529411764705]\n"
     ]
    }
   ],
   "source": [
    "log_cond_prob_class_1 = np.log2(cond_prob_class_1)\n",
    "result_1l = np.sum(log_cond_prob_class_1*new_counts_df.values.T)+np.log2(class_1_prob)\n",
    "result_1l = result_1l[0]\n",
    "\n",
    "\n",
    "log_cond_prob_class_2 = np.log2(cond_prob_class_2)\n",
    "result_2l = np.sum(log_cond_prob_class_2*new_counts_df.values.T)+np.log2(class_2_prob)\n",
    "result_2l = result_2l[0]\n",
    "\n",
    "print(f'Log probabilities Class 1\\n{log_cond_prob_class_1.T}')\n",
    "print(f'Log probabilities Class 2\\n{log_cond_prob_class_2.T}\\n')\n",
    "print(f'Result, Class 1: {result_1l}')\n",
    "print(f'Result, Class 2: {result_2l}')\n",
    "\n",
    "r1 = np.power(2,result_1l)\n",
    "r2 = np.power(2,result_2l)\n",
    "total = r1+r2\n",
    "print([r1/total, r2/total])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
