{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent and Linear Regression: Coding Example\n",
    "\n",
    "Below, we'll use Python to implement **linear regression**. To generate the linear model, we'll use **gradient descent**, which is a general optimization technique but which can find the global optimum in this case. \n",
    "\n",
    "### Linear models\n",
    "\n",
    "In a typical regression problem, we're given a set of training instances, where each instance consists of a set of attributes $x_1$, $x_2$, $\\ldots$, $x_k$, and we are attempting to learn a real-valued function (the **model** or **hypothesis** $h$) from the instances to some target attribute $y$. \n",
    "\n",
    "We can represent each training instance as a row vector $\\mathbf{x} = \\langle x_1,x_2,\\ldots, x_k\\rangle$. Here, to distinguish one training instance from another, we'll use superscripts: E.g.,  $\\mathbf{x}^{(10)}$ refers to training instance \\#10.  Similarly, we can use a superscript to refer to the target value for a training instance (e.g., $y^{(10)}$).  \n",
    "\n",
    "A linear model has the form\n",
    "\n",
    "$h_{\\theta}(\\mathbf{x}) = \\theta_0 + \\displaystyle\\sum_{j=1}^{k} \\theta_j x_j$\n",
    "\n",
    "where each  $\\theta_j$ is a real-valued constant (a **weight**), \n",
    "and $h_{\\theta}(\\mathbf{x})$ is the estimated value of $y$ for instance $\\mathbf{x}$. Note that the function is really determine by the values we choose for each of the $\\theta$s. The subscript in $h_\\theta$ is used to indicate this. \n",
    "\n",
    "\n",
    "If we define $x_0$ to be a constant 1, then the function can be written more succinctly:\n",
    "\n",
    "$h_{\\theta}(\\mathbf{x}) = \\displaystyle\\sum_{j=0}^{k} \\theta_j x_j$\n",
    "\n",
    "Note that the sequence of coefficients $\\theta_0$, $\\ldots$, $\\theta_k$ also forms a vector, and so for any training instance $\\mathbf{x}$, the sum above is just $\\mathbf{\\theta \\cdot x}$ (the dot product of $\\mathbf{\\theta}$ and $\\mathbf{x}$).  \n",
    "\n",
    "\n",
    "$h_{\\theta}(\\mathbf{x})$ deﬁnes a $k$-dimensional **hyperplane**. If $k = 1$, it's a line dividing a 2d space. If $k = 2$, then it's a 2d plane. Intuitively, each weight $\\theta_j$ represents the \"slope\" of the plane in the jth dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cost Function\n",
    "\n",
    "The problem is to ﬁnd values for the weights $\\theta_0$, $\\ldots$ $\\theta_k$ that best *ﬁt* the data. Speciﬁcally, we want to minimize the *mean squared error* between each $y^{(i)}$ and $h_\\theta(x^{(i)})$, where $y^{(i)}$ and $h_\\theta(x^{(i)})$ are the actual and predicted values for example $i$. The term $y^{(i)}-h_\\theta(x^{(i)})$ is called the *residual* of $y^{(i)}$  with respect to $h_\\theta(x^{(i)})$. More specifically, we want to minimize the below *cost function* $J(\\theta)$:\n",
    "\n",
    "$J(\\theta) =  \\frac{1}{2}\\displaystyle\\sum_{i=1}^{n}(h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "\n",
    "Note that $J$ is a funciton of $\\theta$.  The space of possible values for $\\theta$ defines an **error surface**, and we want to find the values for $\\theta$ that correspond to the lowest point on that surface.  Implicitly, $J$ is also a function of the training set, but since we assume that the training set is fixed, we don't explictly symbolize it. \n",
    "\n",
    "Regarding the $\\frac{1}{2}$, we could have just as easily used $\\frac{1}{n}$ instead (where $n$ is the number of instances in the training set), but using $\\frac{1}{2}$ doesn't change what weights yield the best $J$, and it ultimately simpliﬁes calculations (this is described below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Minimizing error is a standard objective in machine learning. For linear regression, it turns out that there is a unique global minimum for $J(\\theta)$, and analytic techniques can be used to ﬁnd it. For other cost functions, however, techniques such as **gradient descent** are more appropriate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In gradient descent, the gradient vector $\\nabla J = \\langle \\frac{\\partial J}{\\partial \\theta_0},\\ldots, \\frac{\\partial J}{\\partial \\theta_k}\\rangle $ at a given point on the error surface (where a point is defined by specific values for $\\theta_0$, $\\ldots$, $\\theta_k$) represents the direction of greatest rate of increase in $J$ at the point. Intuitively,  $\\frac{\\partial J}{\\partial \\theta_j}$\n",
    " at a point on the error surface represents the slope (at that point) of the surface in the jth dimension.\n",
    " \n",
    "The idea behind gradient descent is to adjust the current weights in the opposite direction of the gradient, thereby decreasing the error value. The weights are initialized (typically to 0), and then iteratively adjusted. The equation to **update** each existing weight $\\theta_j$ to  a new value $θ_j'$ is\n",
    "\n",
    "$\\theta_j' := \\theta_j - \\alpha \\frac{\\partial J}{\\partial\\theta_j}$ \n",
    "\n",
    "where $\\alpha$ is a small real-valued constant called the **learning \n",
    "rate**. Note that the weights are updated in parallel rather than sequentially. In a given iteration, updating one weight doesn't affect the updates of the others. \n",
    "\n",
    "If $\\nabla J$ is 0 at a given point (that is, each $\\frac{\\partial J}{\\partial \\theta_i} = 0$), then further updates yield no improvement (this should be obvious given how the update rule is defined). That point constitutes a **local minimum** for $J(\\theta)$. When reached, gradient descent stops. \n",
    "\n",
    "Note that in the context of linear regresssion, the cost function $J$ is **convex**. There is only one minimum (a **global minimum**), and gradient descent can safely be used to ﬁnd it. If the original function to be learned is not linear, however, then the error function might have many local minima. In such cases, gradient descent is not guaranteed to find the global minimum. \n",
    "\n",
    "##### Batch vs Stochastic Gradient Descent\n",
    "\n",
    "In the simplest form of gradient descent, all instances in the data set are examined before updates are made. This is sometimes called **batch gradient descent**. In **stochastic gradient descent**, a randomly chosen instance (or small random sample of instances) is used instead of the entire data set. The gradient is then computed, and the weights are updated  accordingly.  Often, this reduces error more quickly. The algorithm might not converge to a minimum, but the results are typically good approximations.\n",
    "\n",
    "If a good value for $\\alpha$ is chosen, then $J$ should decrease with each iteration, quickly at first but then gradually tapering off.  If a too-large value for $\\alpha$ is used, then $J$ might not converge. Instead, it might increase without bound or oscillate between ponts. If, in contrast, a too-small $\\alpha$ is used, then gradient descent might take a very long time to converge.\n",
    "\n",
    "### Computing the updates\n",
    "\n",
    "In order to update $\\theta_j$ to $\\theta_j'$, we need to compute $\\frac{\\partial J}{\\partial\\theta_j}$, that is $\\frac{\\partial}{\\partial\\theta_j}$ $[ \\frac{1}{2}\\displaystyle\\sum_{i=1}^{n}(h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)})^2]$.  Below are some common rules for computing the derivative $f'(x)$ of a function $f(x)$ of one variable $x$. \n",
    "\n",
    "* If $f(x) = c$, where $c$ is a constant, then $f'(x) = 0$.\n",
    "\n",
    "* If $f(x) = x^n$, then $f'(x) = nx^{n-1}$.\n",
    "\n",
    "* If $f(x) = c g(x)$, where $c$ is a constant, then $f'(x) = c g'(x)$.\n",
    "\n",
    "* If $f(x) = g(x) + h(x)$, then $f'(x) = g'(x) + h'(x)$.\n",
    "\n",
    "* If $f(x) = g(h(x))$, then $f'(x) = g'(h(x))h'(x)$.\n",
    "\n",
    "\n",
    "When computing partial derivatives of functions of multiple variables, e.g.,  $\\frac{\\partial J}{\\partial\\theta_j}$, we treat the other variables ($\\theta_i$ where $i\\neq j$) as constants. \n",
    "\n",
    "Below are steps showing the derivation of $\\frac{\\partial J}{\\partial\\theta_j}$: \n",
    "\n",
    "* $\\frac{\\partial J}{\\partial\\theta_j}$ =\n",
    "\n",
    "* $\\frac{\\partial}{\\partial\\theta_j}$ $[ \\frac{1}{2}\\displaystyle\\sum_{i=1}^{n}(h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)})^2]$ =\n",
    "\n",
    "* $\\frac{1}{2}\\frac{\\partial}{\\partial\\theta_j}$ $[\\displaystyle\\sum_{i=1}^{n}(h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)})^2]$ =\n",
    "\n",
    "* $\\frac{1}{2}\\frac{\\partial}{\\partial\\theta_j}$ $[(h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)})^2$ $+\\ldots+$ $(h_\\theta(\\mathbf{x}^{(n)}) - y^{(n)})^2]$ = \n",
    "\n",
    "* $\\frac{1}{2}$ $[\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)})^2$ $+\\ldots+$ $\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(\\mathbf{x}^{(n)}) - y^{(n)})^2]$ = \n",
    "\n",
    "* $\\frac{1}{2}$ $[2(h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)})\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)})$ $+\\ldots+$ $2(h_\\theta(\\mathbf{x}^{(n)}) - y^{(n)})\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(\\mathbf{x}^{(n)}) - y^{(n)})]$ = \n",
    "\n",
    "* $\\frac{2}{2}$ $[(h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)})\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)})$ $+\\ldots+$ $(h_\\theta(\\mathbf{x}^{(n)}) - y^{(n)})\\frac{\\partial}{\\partial\\theta_j}(h_\\theta(\\mathbf{x}^{(n)}) - y^{(n)})]$ = \n",
    "\n",
    "*  $[(h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)})(\\frac{\\partial}{\\partial\\theta_j} h_\\theta(\\mathbf{x}^{(1)}) - \\frac{\\partial}{\\partial\\theta_j} y^{(1)})$ $+\\ldots+$ $(h_\\theta(\\mathbf{x}^{(n)}) - y^{(n)})(\\frac{\\partial}{\\partial\\theta_j} h_\\theta(\\mathbf{x}^{(n)}) - \\frac{\\partial}{\\partial\\theta_j} y^{(n)})]$ =\n",
    "\n",
    "*  $[(h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)})\\frac{\\partial}{\\partial\\theta_j} h_\\theta(\\mathbf{x}^{(1)})$ $+\\ldots+$ $(h_\\theta(\\mathbf{x}^{(n)}) - y^{(n)})\\frac{\\partial}{\\partial\\theta_j} h_\\theta(\\mathbf{x}^{(n)})]$\n",
    "\n",
    "Note that $h_\\theta(\\mathbf{x}^{(i)})$ = $\\theta_0 + \\theta_1 x_1^{(i)} + \\ldots + \\theta_1 x_k^{(i)}$ and all $\\theta_m$ other than $\\theta_j$ are treated as constants,  and so  $\\frac{\\partial}{\\partial\\theta_j} h_\\theta(\\mathbf{x}^{(i)})$ = $x_j^{(i)}$. From this: \n",
    "\n",
    "* $[(h_\\theta(\\mathbf{x}^{(1)}) - y^{(1)})x_j^{(1)}$ $+\\ldots+$ $(h_\\theta(\\mathbf{x}^{(n)}) - y^{(n)})x_j^{(n)})]$ = \n",
    " \n",
    "* $\\displaystyle\\sum_{i=1}^{n}[(h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)}]$ =\n",
    "\n",
    "* $\\displaystyle\\sum_{i=1}^{n}(\\displaystyle\\sum_{m=0}^{k}{[\\theta_m x_m}^{(i)}] - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "\n",
    "Plugging this result into our update rule yields the following.\n",
    "\n",
    "$\\theta_j' := \\theta_j - \\alpha \\displaystyle\\sum_{i=1}^{n}(\\displaystyle\\sum_{m=0}^{k}{[\\theta_m x_m}^{(i)}] - y^{(i)})x_j^{(i)}$ \n",
    "\n",
    "\n",
    "Note that this is for a single weight $\\theta_j$. We would need to compute a result for all other weights, too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "When fitting a function of several attributes, it's often the case that attributes will have very different magnitudes. This can drastically reduce the efficiency of gradient descent. As such, it is good practice to **scale** the inputs (the target is left alone). Different techniques can be used, but sometimes the standard score is used. Each $x^{(i)}_j$ is replaced with \n",
    "\n",
    "$\\frac{x^{(i)}_j-\\mu_j}{\\sigma_j}$\n",
    "\n",
    "where $\\mu_j$ is the mean of $x_j$ across all instances and $\\sigma_j$ is the standard deviation. In practice, not standardizing the values can make tuning the gradient descent procedure more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Linear Regression with Gradient Descent\n",
    "\n",
    "Below is a sample program implementing linear regression using gradient descent. It is possible to specify the learning rate and number of iterations, as well as whether the input data should be scaled. The function works with Numpy arrays. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data set used for the examples\n",
    "house_data = \"..\\\\data-sets\\\\2015_clarke_co_house_sales.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function creating a linear model using gradient descent. \n",
    "#\n",
    "# input: \n",
    "#   in - training data encoded as an (n x m) numpy array (1 instance per row).\n",
    "#   alpha - the learning rate, a floating point number. \n",
    "#   iterations - the number of iterations to run gradient descent. \n",
    "#\n",
    "# output: the weights (coefficients) of the linear model.  \n",
    "#\n",
    "# For the input, assume that the target attribute is in the last column. \n",
    "def gradient_descent(indata, alpha, iterations, scale=True):\n",
    "    # get the number of rows and columns in the input data. \n",
    "    # get the number of rows and columns in the input data. \n",
    "    (rows, cols) = indata.shape; \n",
    "   \n",
    "    # create a row vector of weights, one for each input column,\n",
    "    # initialized to 0. \n",
    "    weights = np.zeros(cols);\n",
    "    \n",
    "    # 1)Prepend  a column of 1s to the instances (for the y-intercept)\n",
    "    instances = np.hstack((np.ones((rows,1)), indata[:,:-1]))\n",
    "\n",
    "    # 2)Separate the input attributes from the target attribute, forming\n",
    "    # two numpy arrays.\n",
    "    targets = indata[:,-1] \n",
    "\n",
    "    # 3) \n",
    "    # Standardize the inputs. For each value x of an attribute, compute \n",
    "    # (x-mu)/sd, where mu and sd are the mean and standard deviation of values \n",
    "    # for that attribute. \n",
    "    \n",
    "    # scale each column (from index 2 to the end column)\n",
    "    if scale:\n",
    "        stdev = np.std(instances,axis=0);\n",
    "        avg = np.mean(instances,axis=0);\n",
    "        for i in range(1, cols): # skip the ones column\n",
    "            instances[:,i] = (instances[:,i] - avg[i])/stdev[i];\n",
    "\n",
    "    # initialize a list to store the error at each iteration (for graphing). \n",
    "    errList = np.zeros(iterations);\n",
    "    \n",
    "    # run gradient descent for the given number of instances. \n",
    "    for i in range(iterations):\n",
    "        # Compute model for each instance: Multiply instance input matrix\n",
    "        # (n x m) by (m x 1) weight vector to get (n x 1) matrix of predictions.  \n",
    "        model = instances.dot(weights);\n",
    "        \n",
    "        # 5) For each instance D, compute the difference D_i between the target and the predicted value. \n",
    "        diff = (model - targets);\n",
    "        # 6) Update the weights w_1, w_2, ..., w_m. For each column position j: \n",
    "        for j in range(cols):\n",
    "          # 6.1) multiply each instance i's attribute value x_j by D_i\n",
    "          # 6.2) Sum the results of 6.1 together. \n",
    "            dJ = np.sum(diff * instances[:,j])\n",
    "          # 6.3) multiply 6.2's value by alpha and subract the result\n",
    "          # from w_j. This yields the new value for w_j\n",
    "            weights[j] = weights[j] - alpha*dJ\n",
    "        # add the error for this iteration to the stored list of errors.\n",
    "        errList[i] = err(model, targets)\n",
    "    return (weights, errList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean squared error\n",
    "def err(model, targets):\n",
    "    it = np.sum((model - targets)*(model - targets))/len(targets)\n",
    "    return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients: [300804.78600823  85375.98156016   1451.90625206]\n",
      "mse: 57200795712.55842\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(filepath_or_buffer=house_data,header=0)\n",
    "data = df.values\n",
    "(w,e) = gradient_descent(data, 0.0005, 500)\n",
    "scaled_e = e[-1]\n",
    "print(\"coefficients:\", w)\n",
    "print(\"mse:\",scaled_e)\n",
    "plt.plot(e)\n",
    "plt.show()\n",
    "old_e = scaled_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unscaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients: [1.97511987e-01 1.29705073e+02 2.20925354e-02]\n",
      "mse: 58711376606.48234\n",
      "old: 57200795712.55842\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = pd.read_csv(filepath_or_buffer=house_data,header=0)\n",
    "data2 = df2.values\n",
    "(w,e) = gradient_descent(data2, 00.0000000001, 200,scale=False)\n",
    "unscaled_e = e[-1]\n",
    "print(\"coefficients:\", w)\n",
    "print(\"mse:\", unscaled_e)\n",
    "print(\"old:\", old_e)\n",
    "\n",
    "plt.plot(e)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare output to scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit coefficients:  [85375.98156016  1451.90625206]\n",
      "scikit intercept:  300804.78600823047\n",
      "scikit mse: 57200795712.5584\n",
      "hand-rolled mse: 57200795712.55842\n",
      "hand/scikit 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "df3 = pd.read_csv(filepath_or_buffer=house_data,header=0)\n",
    "\n",
    "x_train=df3.values[:,:-1]\n",
    "y_train=df3.values[:,-1]\n",
    "\n",
    "avg = np.mean(x_train,axis=0);\n",
    "stdev = np.std(x_train,axis=0);\n",
    "for i in range(x_train.shape[1]): \n",
    "    x_train[:,i] = (x_train[:,i] - avg[i])/stdev[i]\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "# fit the data\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "# print out the linear model\n",
    "print(\"scikit coefficients: \", regr.coef_)\n",
    "print(\"scikit intercept: \", regr.intercept_)\n",
    "\n",
    "e = metrics.mean_squared_error(y_train,regr.predict(x_train))\n",
    "print(\"scikit mse:\", e)\n",
    "print(\"hand-rolled mse:\",scaled_e)\n",
    "print(\"hand/scikit\", scaled_e/e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:  [ 91.26137924 205.39784564]\n",
      "intercept:  99562.07394192755\n",
      "scikit mse: 57200795712.5584\n",
      "hand-rolled mse: 58711376606.48234\n",
      "hand/scikit 1.026408389518125\n"
     ]
    }
   ],
   "source": [
    "df4 = pd.read_csv(filepath_or_buffer=house_data,header=0)\n",
    "\n",
    "x_train=df4.values[:,:-1]\n",
    "y_train=df4.values[:,-1]\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "# fit the data\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "# print out the linear model\n",
    "print(\"coefficients: \", regr.coef_)\n",
    "print(\"intercept: \", regr.intercept_)\n",
    "#compute predicted values on test test;\n",
    "predicted = regr.predict(x_train);\n",
    "\n",
    "e = metrics.mean_squared_error(y_train,regr.predict(x_train))\n",
    "print(\"scikit mse:\", e)\n",
    "print(\"hand-rolled mse:\",unscaled_e)\n",
    "print(\"hand/scikit\", unscaled_e/e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
