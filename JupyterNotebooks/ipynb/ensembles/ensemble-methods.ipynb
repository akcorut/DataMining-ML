{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np;\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Ensemble Methods\n",
    "\n",
    "We'll go over code for some ensemble methods in `scikit-learn`. These are defined in the module `sklearn.ensemble`. \n",
    "\n",
    "**See:** https://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "### Read in a data set\n",
    "\n",
    "We'll use the `wdbc` breast cancer data set. In this dataset, `B` is 1 and `M` is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "bc=datasets.load_breast_cancer()\n",
    "\n",
    "#iris = datasets.load_iris()\n",
    "\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdbc_data = '../data-sets/wdbc.csv'\n",
    "df = pd.read_csv(wdbc_data, header=0)\n",
    "data = df.iloc[:].values\n",
    "X = data[:, 1:-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data\n",
    "\n",
    "We'll scale the data using a standard scaler and then split the sets into training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 398\n",
      "Testing set size: 171\n",
      "Input attributes: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from  sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "XScaled= scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(XScaled, y, test_size=0.3, shuffle=True)\n",
    "\n",
    "print(\"Training set size:\", len(y_train))\n",
    "print(\"Testing set size:\", len(y_test))\n",
    "print(\"Input attributes:\",X.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "#### Classification\n",
    "\n",
    "Bagging estimators exist for classification and for regression. The estimator for classfication is called `BaggingClassifier`. Note the parameters that can be set: \n",
    "\n",
    "*  `base_estimator`\n",
    "*  `n_estimators`\n",
    "*  `max_samples`\n",
    "*  `max_features`\n",
    "\n",
    "**See:** https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "\n",
    "Below, we'll create a descision tree classifier and compare it to bagged decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "\tB\tM\n",
      "B\t101\t10\n",
      "M\t6\t54\n",
      "--------------------\n",
      "accuracy: 0.9064327485380117\n",
      "\n",
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                        class_weight=None,\n",
      "                                                        criterion='gini',\n",
      "                                                        max_depth=None,\n",
      "                                                        max_features=None,\n",
      "                                                        max_leaf_nodes=None,\n",
      "                                                        min_impurity_decrease=0.0,\n",
      "                                                        min_impurity_split=None,\n",
      "                                                        min_samples_leaf=1,\n",
      "                                                        min_samples_split=2,\n",
      "                                                        min_weight_fraction_leaf=0.0,\n",
      "                                                        presort='deprecated',\n",
      "                                                        random_state=None,\n",
      "                                                        splitter='best'),\n",
      "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "                  max_samples=1.0, n_estimators=25, n_jobs=None,\n",
      "                  oob_score=False, random_state=None, verbose=0,\n",
      "                  warm_start=False)\n",
      "\tB\tM\n",
      "B\t99\t12\n",
      "M\t5\t55\n",
      "--------------------\n",
      "accuracy: 0.9005847953216374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "treeLearner = tree.DecisionTreeClassifier()\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=tree.DecisionTreeClassifier(), \n",
    "                            n_estimators=25, \n",
    "                            max_samples=1.0, \n",
    "                            max_features=1.0)\n",
    "\n",
    "\n",
    "for clf in [treeLearner,bagging]:\n",
    "    clf.fit(X_train, y_train)                         \n",
    "    predicted= np.array(clf.predict(X_test))\n",
    "    cm = metrics.confusion_matrix(y_test, predicted,labels=[1,0])\n",
    "    print()\n",
    "    print(clf)\n",
    "    print(\"\",\"B\",\"M\",sep=\"\\t\")\n",
    "    print(\"B\",cm[0,0],cm[0,1],sep=\"\\t\" )\n",
    "    print(\"M\",cm[1,0],cm[1,1],sep=\"\\t\" )\n",
    "    print('-'*20)\n",
    "    print(\"accuracy:\", metrics.accuracy_score(predicted,y_test))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "For regression, `BaggingRegressor` can be used. We'll try it with a California housing data set. \n",
    "\n",
    "**See:** https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14448\n",
      "Testing set size: 6192\n",
      "Input attributes: 8\n",
      "--------------------\n",
      "<class 'sklearn.tree._classes.DecisionTreeRegressor'>\t MSE:0.5347956993773441\n",
      "<class 'sklearn.ensemble._bagging.BaggingRegressor'>\t MSE:0.26757225464244205\n",
      "--------------------\n",
      "2.0232000000000006 2.443\n",
      "1.7665599999999997 1.729\n",
      "1.10364 0.862\n",
      "0.75464 0.611\n",
      "0.85328 0.868\n",
      "2.3042800000000003 3.257\n",
      "2.14224 2.295\n",
      "2.39656 3.229\n",
      "0.76824 0.643\n",
      "2.44276 1.975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import tree\n",
    "import scipy.stats\n",
    "\n",
    "treeLearner = tree.DecisionTreeRegressor()\n",
    "\n",
    "bagging = BaggingRegressor(base_estimator=tree.DecisionTreeRegressor(), \n",
    "                            n_estimators=25, \n",
    "                            max_samples=1.0, \n",
    "                            max_features=1.0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "cali= datasets.fetch_california_housing()\n",
    "XCaliScaled= scaler.fit_transform(cali.data)\n",
    "\n",
    "X_cali_train, X_cali_test, y_cali_train, y_cali_test =  train_test_split(XCaliScaled, cali.target, test_size=0.3, shuffle=True)\n",
    "\n",
    "print(\"Training set size:\", len(y_cali_train))\n",
    "print(\"Testing set size:\", len(y_cali_test))\n",
    "print(\"Input attributes:\",XCaliScaled.shape[1])\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "for clf in [treeLearner,bagging]:\n",
    "    clf.fit(X_cali_train, y_cali_train)                         \n",
    "    predicted= np.array(clf.predict(X_cali_test))\n",
    "    print(f\"{clf.__class__}\\t MSE:{metrics.mean_squared_error(y_cali_test,predicted)}\")\n",
    "\n",
    "print('-'*20)\n",
    "    \n",
    "    \n",
    "for i in range(10):\n",
    "    print(clf.predict([X_cali_test[i]])[0], y_cali_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=20640, minmax=(0.14999, 5.00001), mean=2.068558169089147, variance=1.3316148163035277, skewness=0.9776922140978419, kurtosis=0.3275001388119616)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.describe(cali.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "**See:** https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'sklearn.tree._classes.DecisionTreeClassifier'>\n",
      "\tB\tM\n",
      "B\t101\t10\n",
      "M\t8\t52\n",
      "--------------------\n",
      "accuracy: 0.8947368421052632\n",
      "\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\tB\tM\n",
      "B\t106\t5\n",
      "M\t5\t55\n",
      "--------------------\n",
      "accuracy: 0.9415204678362573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier    \n",
    "from sklearn import tree\n",
    "\n",
    "treeLearner = tree.DecisionTreeClassifier()\n",
    "\n",
    "randomF = RandomForestClassifier(n_estimators=50, \n",
    "                            criterion='gini', # or entropy\n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            max_features='auto', \n",
    "                            bootstrap=True)\n",
    "\n",
    "for clf in [treeLearner,randomF]:\n",
    "    clf.fit(X_train, y_train)                         \n",
    "    predicted= np.array(clf.predict(X_test))\n",
    "    cm = metrics.confusion_matrix(y_test, predicted,labels=[1,0])\n",
    "    print(f\"\\n{clf.__class__}\")\n",
    "    print(\"\",\"B\",\"M\",sep=\"\\t\")\n",
    "    print(\"B\",cm[0,0],cm[0,1],sep=\"\\t\" )\n",
    "    print(\"M\",cm[1,0],cm[1,1],sep=\"\\t\" )\n",
    "    print('-'*20)\n",
    "    print(\"accuracy:\", metrics.accuracy_score(predicted,y_test))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "**See:** https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14448\n",
      "Testing set size: 6192\n",
      "Input attributes: 8\n",
      "DescribeResult(nobs=20640, minmax=(0.14999, 5.00001), mean=2.068558169089147, variance=1.3316148163035277, skewness=0.9776922140978419, kurtosis=0.3275001388119616)\n",
      "--------------------\n",
      "MSE: 0.5622960422606792\n",
      "MSE: 0.2576759036837204\n",
      "--------------------\n",
      "3.0213203999999996 3.31\n",
      "0.88696 0.95\n",
      "1.889720000000001 4.5\n",
      "1.7393600000000005 1.708\n",
      "1.7983199999999997 2.215\n",
      "1.9486199999999996 2.308\n",
      "2.7991601999999993 4.029\n",
      "2.3649199999999997 2.151\n",
      "1.8054400000000006 1.648\n",
      "1.1465000000000003 1.264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "import scipy.stats\n",
    "\n",
    "treeLearner = tree.DecisionTreeRegressor()\n",
    "\n",
    "bagging = RandomForestRegressor(n_estimators=50, \n",
    "                            criterion='mse', # mae\n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            max_features='auto', \n",
    "                            bootstrap=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "XCaliScaled= scaler.fit_transform(cali.data)\n",
    "\n",
    "X_cali_train, X_cali_test, y_cali_train, y_cali_test =  train_test_split(XCaliScaled, cali.target, test_size=0.3, shuffle=True)\n",
    "\n",
    "print(\"Training set size:\", len(y_cali_train))\n",
    "print(\"Testing set size:\", len(y_cali_test))\n",
    "print(\"Input attributes:\",XCaliScaled.shape[1])\n",
    "print(scipy.stats.describe(cali.target))\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "\n",
    "for clf in [treeLearner,bagging]:\n",
    "    clf.fit(X_cali_train, y_cali_train)                         \n",
    "    predicted= np.array(clf.predict(X_cali_test))\n",
    "    print(\"MSE:\",metrics.mean_squared_error(y_cali_test,predicted))\n",
    "\n",
    "print('-'*20)\n",
    "    \n",
    "for i in range(10):\n",
    "    print(clf.predict([X_cali_test[i]])[0], y_cali_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees\n",
    "\n",
    "Here, random candidate splits are generated, and the best amoung these is chosen. \n",
    "\n",
    "**See:** https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tB\tM\n",
      "B\t101\t10\n",
      "M\t6\t54\n",
      "--------------------\n",
      "accuracy: 0.9064327485380117\n",
      "\tB\tM\n",
      "B\t108\t3\n",
      "M\t4\t56\n",
      "--------------------\n",
      "accuracy: 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier    \n",
    "from sklearn import tree\n",
    "\n",
    "treeLearner = tree.DecisionTreeClassifier()\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=50, \n",
    "                            criterion='gini', \n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            max_features='auto', \n",
    "                            bootstrap=True)\n",
    "\n",
    "for clf in [treeLearner,et]:\n",
    "    clf.fit(X_train, y_train)                         \n",
    "    predicted= np.array(clf.predict(X_test))\n",
    "    cm = metrics.confusion_matrix(y_test, predicted,labels=[1,0])\n",
    "    print(\"\",\"B\",\"M\",sep=\"\\t\")\n",
    "    print(\"B\",cm[0,0],cm[0,1],sep=\"\\t\" )\n",
    "    print(\"M\",cm[1,0],cm[1,1],sep=\"\\t\" )\n",
    "    print('-'*20)\n",
    "    print(\"accuracy:\", metrics.accuracy_score(predicted,y_test))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "#### Classification\n",
    "\n",
    "**See:** https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "Regarding the algorithm used: \n",
    "\n",
    "* \"If `SAMME.R` then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities.\" \n",
    "* \"If `SAMME` then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tB\tM\n",
      "B\t101\t10\n",
      "M\t5\t55\n",
      "--------------------\n",
      "accuracy: 0.9122807017543859\n",
      "\tB\tM\n",
      "B\t105\t6\n",
      "M\t6\t54\n",
      "--------------------\n",
      "accuracy: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier  \n",
    "from sklearn import tree\n",
    "\n",
    "treeLearner = tree.DecisionTreeClassifier()\n",
    "\n",
    "ad = AdaBoostClassifier( base_estimator =tree.DecisionTreeClassifier(max_depth=5),\n",
    "                           n_estimators=25, \n",
    "                           learning_rate=1.0, \n",
    "                           algorithm='SAMME.R')\n",
    "\n",
    "for clf in [treeLearner,ad]:\n",
    "    clf.fit(X_train, y_train)                         \n",
    "    predicted= np.array(clf.predict(X_test))\n",
    "    cm = metrics.confusion_matrix(y_test, predicted,labels=[1,0])\n",
    "    print(\"\",\"B\",\"M\",sep=\"\\t\")\n",
    "    print(\"B\",cm[0,0],cm[0,1],sep=\"\\t\" )\n",
    "    print(\"M\",cm[1,0],cm[1,1],sep=\"\\t\" )\n",
    "    print('-'*20)\n",
    "    print(\"accuracy:\", metrics.accuracy_score(predicted,y_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14448\n",
      "Testing set size: 6192\n",
      "Input attributes: 8\n",
      "DescribeResult(nobs=20640, minmax=(0.14999, 5.00001), mean=2.068558169089147, variance=1.3316148163035277, skewness=0.9776922140978419, kurtosis=0.3275001388119616)\n",
      "--------------------\n",
      "MSE: 0.5500306827816238\n",
      "MSE: 0.24607775772125112\n",
      "--------------------\n",
      "[1.219] 1.25\n",
      "[2.693] 3.124\n",
      "[3.647] 5.00001\n",
      "[0.946] 0.838\n",
      "[0.913] 1.408\n",
      "[1.164] 1.306\n",
      "[1.581] 1.498\n",
      "[0.9] 1.333\n",
      "[0.863] 0.691\n",
      "[4.847] 4.373\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import tree\n",
    "import scipy.stats\n",
    "\n",
    "treeLearner = tree.DecisionTreeRegressor()\n",
    "\n",
    "bagging = AdaBoostRegressor( base_estimator =tree.DecisionTreeRegressor(),\n",
    "                           n_estimators=25, \n",
    "                           learning_rate=1.0, \n",
    "                           loss='square')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "XCaliScaled= scaler.fit_transform(cali.data)\n",
    "\n",
    "X_cali_train, X_cali_test, y_cali_train, y_cali_test =  train_test_split(XCaliScaled, cali.target, test_size=0.3, shuffle=True)\n",
    "\n",
    "print(\"Training set size:\", len(y_cali_train))\n",
    "print(\"Testing set size:\", len(y_cali_test))\n",
    "print(\"Input attributes:\",XCaliScaled.shape[1])\n",
    "print(scipy.stats.describe(cali.target))\n",
    "\n",
    "print('-'*20)\n",
    "\n",
    "\n",
    "for clf in [treeLearner,bagging]:\n",
    "    clf.fit(X_cali_train, y_cali_train)                         \n",
    "    predicted= np.array(clf.predict(X_cali_test))\n",
    "    print(\"MSE:\",metrics.mean_squared_error(y_cali_test,predicted))\n",
    "\n",
    "print('-'*20)\n",
    "    \n",
    "for i in range(10):\n",
    "    print(clf.predict([X_cali_test[i]]), y_cali_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "\n",
    "Votiting classifier and regressor classes are also defined in sklearn. \n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\tB\tM\n",
      "B\t110\t1\n",
      "M\t3\t57\n",
      "--------------------\n",
      "accuracy: 0.9766081871345029\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\tB\tM\n",
      "B\t104\t7\n",
      "M\t6\t54\n",
      "--------------------\n",
      "accuracy: 0.9239766081871345\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'sklearn.tree._classes.DecisionTreeClassifier'>\n",
      "\tB\tM\n",
      "B\t101\t10\n",
      "M\t7\t53\n",
      "--------------------\n",
      "accuracy: 0.9005847953216374\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'sklearn.ensemble._voting.VotingClassifier'>\n",
      "\tB\tM\n",
      "B\t104\t7\n",
      "M\t5\t55\n",
      "--------------------\n",
      "accuracy: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n",
    "                          random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = tree.DecisionTreeClassifier()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('dt', clf3)], voting='hard')\n",
    "\n",
    "for clf in [clf1, clf2, clf3, eclf]:\n",
    "    clf.fit(X_train, y_train)                         \n",
    "    predicted= np.array(clf.predict(X_test))\n",
    "    cm = metrics.confusion_matrix(y_test, predicted,labels=[1,0])\n",
    "    print('-'*80)\n",
    "    print(clf.__class__)\n",
    "    print(\"\",\"B\",\"M\",sep=\"\\t\")\n",
    "    print(\"B\",cm[0,0],cm[0,1],sep=\"\\t\" )\n",
    "    print(\"M\",cm[1,0],cm[1,1],sep=\"\\t\" )\n",
    "    print('-'*20)\n",
    "    print(\"accuracy:\", metrics.accuracy_score(predicted,y_test))   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
