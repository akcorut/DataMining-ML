{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief introduction to NLTK\n",
    "\n",
    "NLTK (the Natural Language Toookit) is a collection of Python libraries for processing text. Tokenizers, word-stemmers and lemmatizers, part-of-speech taggers, and many other tools are defined. \n",
    "\n",
    "* **Website**: https://www.nltk.org/ \n",
    "* **Free book**: \tSteven Bird, Ewan Klein, and Edward Loper. *Natural Language Processing with Python -- Analyzing Text with the Natural Language Toolkit*. https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nimda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nimda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nimda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nimda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\nimda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These will need to be downloaded.\n",
    "# This only needs to be run once. \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_default = stopwords.words('english')\n",
    "\n",
    "def read_file(f, lower=True):\n",
    "    \"\"\"Simple wrapper to read a file.\"\"\"\n",
    "    with open(f) as reader:\n",
    "        text = reader.read()\n",
    "        if lower:\n",
    "            return text.lower()\n",
    "        return text\n",
    "\n",
    "def get_tokens(text):\n",
    "    tokens= text.split()\n",
    "    return cleanup(tokens)\n",
    "    \n",
    "def cleanup(tokens, nopunct=True, nowhite=True, nonum=True, stop=stop_default):\n",
    "    \"\"\"Use some basic python to clean up tokens.\n",
    "    stopwords, punctuation, digits are removed.\"\"\"\n",
    "\n",
    "    toremove=\"\"\n",
    "    if nopunct:\n",
    "        toremove = toremove + string.punctuation\n",
    "    if nonum:\n",
    "        toremove = toremove + \"0123456789\"\n",
    "    if nowhite:\n",
    "        toremove = toremove + string.whitespace\n",
    "    if toremove:\n",
    "        tab = \"\".maketrans(\"\", \"\", toremove)\n",
    "        tokens = [t.translate(tab).strip() for t in tokens]\n",
    "        tokens = [t for t in tokens if t]\n",
    "    return remove_tokens(tokens, stop)\n",
    "\n",
    "def remove_tokens(tokens, stopwords):\n",
    "    \"\"\"Takes a list of tokens and returns a list with provided stopwords removed.\"\"\"\n",
    "    return [t for t in tokens if t not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to our earlier method of tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization_2(sent):\n",
    "    \"\"\"Split text into sentences using NLTK sentence tokenizer.\n",
    "    Print out 10 sentences, starting at index 1000. \n",
    "    \"\"\"\n",
    "    words = word_tokenize(sent)\n",
    "    for w in words:\n",
    "        print(f'<{w}>')\n",
    "            \n",
    "def stem(tokens):\n",
    "    \"\"\"Using a Porter Stemmer\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(tok) for tok in tokens]\n",
    "\n",
    "def lemmatizer(tokens, pos= ['n', 'v', 'a', 'r']):\n",
    "    \"\"\"Using a WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    lem = WordNetLemmatizer()    \n",
    "    for p in pos:\n",
    "        print(p)\n",
    "        print('-'*30)\n",
    "        lemmatized = [(tok,lem.lemmatize(tok,pos=p)) for tok in tokens]\n",
    "        for (tok,ste) in lemmatized:\n",
    "            X = 'X'\n",
    "            if tok == ste:\n",
    "                X = ''\n",
    "            print(f'{X}\\t{tok}\\t->\\t{ste}')\n",
    "            \n",
    "def pos(tokens, as_string=False):\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    result = tagged\n",
    "    if as_string:\n",
    "        result = ['__'.join(pair) for pair in tagged]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def process_file(file, n=20, lower=True, tokenizer=None):\n",
    "    # read file\n",
    "    text = read_file(file, lower = lower)\n",
    "\n",
    "    # tokenize file\n",
    "    \n",
    "    if not tokenizer:\n",
    "        tokens= get_tokens(text)\n",
    "    else:\n",
    "        tokens = tokenizer(text)\n",
    "\n",
    "    # determine number of rows given bucket size\n",
    "    rows = len(tokens) // n\n",
    "\n",
    "    # truncate input to ensure even splits\n",
    "    tokens = tokens[:(rows*n)]\n",
    "    \n",
    "    # use numpy to reshape input into rows of n tokens\n",
    "    country = np.array(tokens)\n",
    "    country = country.reshape((rows,n))\n",
    "    rows, columns = country.shape\n",
    "    \n",
    "    # recombine each row into a string. \n",
    "    results = [0] * rows\n",
    "    for i in range(rows):\n",
    "        results[i] = ' '.join(country[i])\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_files(files, n = 50):\n",
    "    dataframes = []\n",
    "    for [f,title,author] in files:\n",
    "        working_file = working_dir/f\n",
    "        strings = process_file(working_file, n=n)    \n",
    "        df = pd.DataFrame(strings, columns=['text'])\n",
    "        df['author'] = author\n",
    "        df['title'] = title\n",
    "        dataframes.append(df)\n",
    "    corpus_data = pd.concat(dataframes)\n",
    "    return corpus_data\n",
    "\n",
    "working_dir = Path(\"C://Users//nimda//Desktop//CSCI 6380 Data Mining//hw//hw2//raw\")\n",
    "\n",
    "train_files =  [[\"house_of_the_7_gables.txt\",\"The House of the Seven Gables\",\"Nathaniel Hawthorne\"],\n",
    "            [\"moby_dick.txt\",\"Moby Dick\",\"Herman Melville\"],\n",
    "            [\"mosses.txt\",\"Mosses from an Old Manse and Other Stories\",\"Nathaniel Hawthorne\"],\n",
    "            [\"piazza_tales.txt\",\"The Piazza Tales\",\"Herman Melville\"],\n",
    "            [\"return_sherlock.txt\",\"The Return of Sherlock Holmes\",\"Arthur Conan Doyle\"],\n",
    "            [\"scarlett_letter.txt\",\"The Scarlet Letter\",\"Nathaniel Hawthorne\"],\n",
    "            [\"white_company.txt\",\"The White Company\",\"Arthur Conan Doyle\"],\n",
    "            [\"white_jacket.txt\",\"White Jacket\",\"Herman Melville\"]]\n",
    "\n",
    "test_files = [  [\"baskervilles.txt\",\"The Hound of the Baskervilles\",\"Arthur Conan Doyle\"],\n",
    "                [\"blithedale.txt\",\"The Blithedale Romance\",\"Nathaniel Hawthorne\"],\n",
    "                [\"typee.txt\",\"Typee\",\"Herman Melville\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "import numpy as np\n",
    "\n",
    "def naive_bayes(n = 25):\n",
    "    train_df = process_files(train_files, n = n)\n",
    "    test_df = process_files(test_files, n = n)\n",
    "    \n",
    "    X_train= train_df['text']\n",
    "    targets_train = train_df['author']\n",
    "    X_test= test_df['text']\n",
    "    targets_test = test_df['author']\n",
    "    \n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_train)\n",
    "    X_new_counts = count_vect.transform(X_test)\n",
    "\n",
    "    clfMNB = MultinomialNB().fit(X_train_counts, targets_train)\n",
    "    predicted = clfMNB.predict(X_new_counts)    \n",
    "    mnb = np.mean(predicted == targets_test)\n",
    "\n",
    "    clfBNB = BernoulliNB().fit(X_train_counts, targets_train)\n",
    "    predicted = clfBNB.predict(X_new_counts)    \n",
    "    bnb = np.mean(predicted == targets_test)\n",
    "    \n",
    "    print(f\"N={n} MultinomialNB: {mnb}; BernoulliNB: {bnb}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=10 MultinomialNB: 0.6599951183793019; BernoulliNB: 0.6679684321861524\n",
      "N=25 MultinomialNB: 0.7414038657171923; BernoulliNB: 0.7611393692777213\n",
      "N=50 MultinomialNB: 0.8017908017908018; BernoulliNB: 0.833943833943834\n",
      "N=250 MultinomialNB: 0.8938775510204081; BernoulliNB: 0.9346938775510204\n",
      "N=500 MultinomialNB: 0.9139344262295082; BernoulliNB: 0.9590163934426229\n",
      "N=1000 MultinomialNB: 0.9504132231404959; BernoulliNB: 0.9834710743801653\n"
     ]
    }
   ],
   "source": [
    "for n in [10,25,50,250,500,1000]:\n",
    "    naive_bayes(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
